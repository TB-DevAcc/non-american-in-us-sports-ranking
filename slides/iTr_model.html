<section class="iTr-model-slide" data-menu-title="iTransformer Model Slide">
  <div class="slide-container">
    <h2>iTransformer Model Architecture</h2>
    <div class="container-fluid d-flex align-items-center">
      <div class="row align-items-center justify-content-center">
        <div class="col-3">
          <ul class="list-group" id="header-list-group">
            <li class="fragment mt-3">
              <strong>Architecture</strong>
              <div>Encoder-only, leverages Transformer</div>
            </li>
            <li class="fragment mt-3">
              <strong>Tokenization</strong>
              <div>Treats time series as independent tokens</div>
            </li>
            <li class="fragment mt-3">
              <strong>Self-Attention</strong>
              <div>Captures multivariate correlations efficiently</div>
            </li>
            <li class="fragment mt-3">
              <strong>Feed-Forward Networks</strong>
              <div>Transforms series into insightful representations</div>
            </li>
          </ul>
        </div>
        <figure class="col-9 figure fragment">
          <img src="src/img/iT_motivation.svg" alt="iTransformer Architecture"
            class="img-fluid mx-auto figure-image img-thumbnail">
          <figcaption class="figure-caption">
            Diagram of iTransformer architecture highlighting input handling through self-attention and feed-forward
            layers to output<span class="citation" data-ref="liu_ITransformerInvertedTransformers_2024"></span>.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="iTr-model-slide" data-menu-title="iTransformer Model Architecture Detailed">
  <div class="slide-container">
    <h2>Detailed Architecture of iTransformer</h2>
    <div class="fig-container container-fluid d-flex align-items-center" data-preload
      data-file="src/d3/iT_architecture.html" data-style="height: 100%"></div>
  </div>
</section>

<section class="iTr-model-slide" data-menu-title="iTransformer Foundation">
  <div class="slide-container">
    <h2>iTransformer Model Foundation</h2>
    <div class="container-fluid">
      <div class="row">
        <div class="col-12 fragment">
          <p>No special requirements are necessary for Transformer variants, enabling a range of efficient
            attention mechanisms.</p>
        </div>
        <div class="col-6 fragment">
          <h3>Core Model</h3>
          <p>Predicting future series for specific variate:</p>
          <div class="equation">
            \[
            \hat{Y}_{:,n} = \text{Projection}(h^{L}_{n}) \quad \text{where} \quad h^{0}_{n} = \text{Embedding}(X_{:,n})
            \]
          </div>
          <p>Where:</p>
          <ul>
            <li>\( H^{l+1} = \text{TrmBlock}(H^l) \) for \( l=0,\ldots,L-1 \)</li>
            <li>\( H = \{h_1, \ldots, h_N\} \in \mathbb{R}^{N \times D} \)</li>
            <li>Embedding and Projection are MLPs</li>
            <li>Variate tokens interact via self-attention within each TrmBlock</li>
          </ul>
        </div>
        <div class="col-6 fragment">
          <h3>Training and Efficiency</h3>
          <figure class="figure mt-0">
            <img id="iT-eff-img" src="src/img/iT_eff_left.svg" alt="iTransformer Training"
              class="img-fluid mx-auto figure-image img-thumbnail">
            <figcaption class="figure-caption">
              Model is trained on arbitrary numbers of variates, allowing flexibility from training to inference
              phases&NoBreak;<span class="citation" data-ref="liu_ITransformerInvertedTransformers_2024"></span>.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>