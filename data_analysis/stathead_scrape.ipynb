{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log in to the website. Proceed to the next cell once logged in.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Setup WebDriver\n",
    "service = Service(\"/opt/homebrew/Caskroom/chromedriver/125.0.6422.60/chromedriver-mac-arm64/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Navigate to the login page\n",
    "driver.get(\"https://stathead.com/users/login.cgi\")  # Update if the login URL differs\n",
    "print(\"Please log in to the website. Proceed to the next cell once logged in.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "def fetch_csv_data(url):\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10)  # Setup wait for up to 10 seconds\n",
    "\n",
    "    # Check for \"no results\" message before proceeding\n",
    "    no_results_text = \"Sorry, there are no results for your search.\"\n",
    "    page_content = driver.page_source\n",
    "    if no_results_text in page_content:\n",
    "        print(\"No more results to fetch.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Find and hover over the parent element to make the export button visible\n",
    "        hover_element = wait.until(EC.visibility_of_element_located((By.XPATH, '//li[@class=\"hasmore\"]/span[text()=\"Export Data\"]')))\n",
    "        ActionChains(driver).move_to_element(hover_element).perform()\n",
    "\n",
    "        # Now wait for the specific export button to become clickable after the hover\n",
    "        export_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@class=\"tooltip\" and contains(@tip, \"suitable for use with Excel\") and text()=\"Get table as CSV (for Excel)\"]')))\n",
    "        export_button.click()\n",
    "\n",
    "        # Wait for the CSV data to become visible\n",
    "        wait.until(EC.visibility_of_element_located((By.ID, 'csv_stats')))\n",
    "    except TimeoutException as e:\n",
    "        print(\"Timeout while waiting for elements:\", e)\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(\"Error interacting with page elements:\", e)\n",
    "        return \"\"\n",
    "\n",
    "    # Extract CSV data\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    pre_tag = soup.find('pre', id='csv_stats')\n",
    "    return pre_tag.text if pre_tag else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more results to fetch.\n",
      "Finished fetching all available data.\n"
     ]
    }
   ],
   "source": [
    "# NBA foreign\n",
    "# base_url = \"https://stathead.com/basketball/player-season-finder.cgi?request=1&draft_pick_type=overall&comp_type=reg&order_by=ws&match=player_season&season_start=1&display_type=totals&season_end=-1&locationMatch=isnot&pob=USA&comp_id=NBA&year_min=1947&offset=\"\n",
    "# NBA all\n",
    "# base_url = \"https://stathead.com/basketball/player-season-finder.cgi?request=1&draft_pick_type=overall&comp_type=reg&order_by=ws&match=player_season&season_start=1&year_max=2024&display_type=totals&season_end=-1&comp_id=NBA&year_min=1947&offset=\"\n",
    "# NBA height\n",
    "# base_url = \"https://stathead.com/basketball/player-season-finder.cgi?request=1&draft_pick_type=overall&comp_type=reg&order_by=height&match=player_season&season_start=1&display_type=totals&season_end=-1&locationMatch=isnot&pob=USA&comp_id=NBA&year_min=1947&offset=\"\n",
    "# NBA weight\n",
    "# base_url = \"https://stathead.com/basketball/player-season-finder.cgi?request=1&draft_pick_type=overall&comp_type=reg&order_by=weight&match=player_season&season_start=1&display_type=totals&season_end=-1&locationMatch=isnot&pob=USA&comp_id=NBA&year_min=1947&offset=\"\n",
    "# NFL all\n",
    "base_url = \"https://stathead.com/football/player-season-finder.cgi?request=1&draft_pick_type=overall&comp_type=reg&order_by=av&match=player_season&season_start=1&season_end=-1&weight_max=500&rookie=N&year_min=1999&cstat[1]=pass_cmp&ccomp[1]=gt&cval[1]=0&cstat[2]=rush_att&ccomp[2]=gt&cval[2]=0&cstat[3]=targets&ccomp[3]=gt&cval[3]=0&cstat[4]=all_td&ccomp[4]=gt&cval[4]=0&cstat[5]=sacks&ccomp[5]=gt&cval[5]=0&cstat[6]=def_int&ccomp[6]=gt&cval[6]=0&cstat[7]=punt&ccomp[7]=gt&cval[7]=0&cstat[8]=touches&ccomp[8]=gt&cval[8]=0&cstat[9]=fumbles_forced&ccomp[9]=gt&cval[9]=0&offset=\"\n",
    "# NFL Measurables\n",
    "# base_url = \"https://stathead.com/football/player-season-finder.cgi?request=1&draft_pick_type=overall&comp_type=reg&order_by=height&match=player_season_combined&season_start=1&season_end=-1&weight_max=500&rookie=N&offset=\"\n",
    "# MLB Batting foreign\n",
    "# base_url = \"https://stathead.com/baseball/player-batting-season-finder.cgi?request=1&draft_pick_type=overall&location=pob&comp_type=reg&order_by=b_war&match=player_season&season_start=1&weight_max=500&season_end=-1&locationMatch=isnot&pob=USA&exactness=anymarked&offset=\"\n",
    "# MLB Batting Height\n",
    "# base_url = \"https://stathead.com/baseball/player-batting-season-finder.cgi?request=1&draft_pick_type=overall&location=pob&comp_type=reg&order_by=height&match=player_season&season_start=1&weight_max=500&season_end=-1&locationMatch=isnot&pob=USA&exactness=anymarked&year_min=1871&offset=\"\n",
    "# MLB Batting Weight\n",
    "# base_url = \"https://stathead.com/baseball/player-batting-season-finder.cgi?request=1&draft_pick_type=overall&location=pob&comp_type=reg&order_by=weight&match=player_season&season_start=1&weight_max=500&season_end=-1&locationMatch=isnot&pob=USA&exactness=anymarked&year_min=1871&offset=\"\n",
    "# MLB Pitching foreign\n",
    "# base_url = \"https://stathead.com/baseball/player-pitching-season-finder.cgi?request=1&draft_pick_type=overall&location=pob&comp_type=reg&order_by=p_war&match=player_season&season_start=1&p_g=x&weight_max=500&season_end=-1&locationMatch=isnot&pob=USA&offset=\"\n",
    "# MLB Pitching height\n",
    "# base_url = \"https://stathead.com/baseball/player-pitching-season-finder.cgi?request=1&draft_pick_type=overall&location=pob&comp_type=reg&order_by=height&match=player_season&season_start=1&p_g=x&weight_max=500&season_end=-1&locationMatch=isnot&pob=USA&year_min=1871&offset=\"\n",
    "# MLB Pitching weight\n",
    "# base_url = \"https://stathead.com/baseball/player-pitching-season-finder.cgi?request=1&draft_pick_type=overall&location=pob&comp_type=reg&order_by=weight&match=player_season&season_start=1&p_g=x&weight_max=500&season_end=-1&locationMatch=isnot&pob=USA&year_min=1871&offset=\"\n",
    "# MLS all\n",
    "# base_url = \"https://stathead.com/fbref/player-season-finder.cgi?request=1&height_type=height_feet&force_min_year=1&weight_min=10&comp_type=c-22&order_by=age&match=player_season&per90_type=player&height_min=60&weight_max=450&phase_id=0&comp_gender=m&per90min_val=5&weight_type=lbs&year_min=1996&offset=\"\n",
    "# MLS foreign\n",
    "# base_url = \"https://stathead.com/fbref/player-season-finder.cgi?request=1&height_type=height_feet&force_min_year=1&height_max=84&comp_type=c-22&order_by=plus_minus&match=player_season&per90_type=player&weight_max=500&locationMatch=isnot&pob=USA&phase_id=0&comp_gender=m&per90min_val=5&weight_type=lbs&cstat[1]=assisted_shots&ccomp[1]=gt&cval[1]=0&cstat[2]=tackles_won&ccomp[2]=gt&cval[2]=0&cstat[3]=minutes_per_game&ccomp[3]=gt&cval[3]=0&cstat[4]=fouls&ccomp[4]=gt&cval[4]=0&cstat[5]=on_xg_for&ccomp[5]=gt&cval[5]=0&offset=\"\n",
    "# NHL foreign\n",
    "# base_url = \"https://stathead.com/hockey/player-season-finder.cgi?request=1&draft_pick_type=overall&comp_type=reg&order_by=goals&match=player_season&season_start=1&season_end=-1&locationMatch=isnot&pob=USA&rookie=N&pos=S&comp_id=NHL&cstat[1]=ops&ccomp[1]=gt&cval[1]=0&cstat[2]=goals_per_game&ccomp[2]=gt&cval[2]=0&offset=\"\n",
    "# NHL height\n",
    "# base_url = \"https://stathead.com/hockey/player-season-finder.cgi?request=1&draft_pick_type=overall&comp_type=reg&order_by=goals&match=player_season&season_start=1&season_end=-1&locationMatch=isnot&pob=USA&rookie=N&pos=S&comp_id=NHL&cstat[1]=ops&ccomp[1]=gt&cval[1]=0&cstat[2]=goals_per_game&ccomp[2]=gt&cval[2]=0&offset=\"\n",
    "offset = 0\n",
    "raw_data_list = []\n",
    "\n",
    "while True:\n",
    "    url = f\"{base_url}{offset}\"\n",
    "    csv_data = fetch_csv_data(url)\n",
    "    if csv_data is None or \"Sorry, there are no results for your search.\" in csv_data:\n",
    "        print(\"Finished fetching all available data.\")\n",
    "        break\n",
    "    raw_data_list.append(csv_data)\n",
    "    offset += 200\n",
    "\n",
    "# Combine all fetched data into one large string\n",
    "all_csv_data = \"\\n\".join(raw_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_data(all_csv_data):\n",
    "    lines = all_csv_data.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    header_found = False\n",
    "    for line in lines:\n",
    "        if 'Rk,Player,' in line and header_found:\n",
    "            continue  # Skip duplicate headers\n",
    "        elif '--- When using SR' in line or '</' in line or '<a href' in line:\n",
    "            continue  # Skip lines containing footer content or HTML tags\n",
    "        cleaned_lines.append(line)\n",
    "        if 'Rk,Player,' in line:\n",
    "            header_found = True  # Mark header as found\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "cleaned_csv = clean_csv_data(all_csv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_1511/2568451621.py:1: DtypeWarning: Columns (0,2,3,4,5,6,7,8,9,10,11,12,13,15,16,40,41,42,43,44,45,46,56,83,84,87,90,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv(StringIO(cleaned_csv))#[['Player', 'Player-additional', 'Wt.']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>PAT.1</th>\n",
       "      <th>Unnamed: 87</th>\n",
       "      <th>FG</th>\n",
       "      <th>FG.1</th>\n",
       "      <th>Unnamed: 90</th>\n",
       "      <th>Unnamed: 91</th>\n",
       "      <th>Unnamed: 92</th>\n",
       "      <th>Unnamed: 93</th>\n",
       "      <th>Unnamed: 94</th>\n",
       "      <th>-additional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rk</td>\n",
       "      <td>Player</td>\n",
       "      <td>AV</td>\n",
       "      <td>Cmp</td>\n",
       "      <td>Att</td>\n",
       "      <td>Tgt</td>\n",
       "      <td>TD</td>\n",
       "      <td>Sk</td>\n",
       "      <td>Int</td>\n",
       "      <td>Pnt</td>\n",
       "      <td>...</td>\n",
       "      <td>XPA</td>\n",
       "      <td>XP%</td>\n",
       "      <td>FGM</td>\n",
       "      <td>FGA</td>\n",
       "      <td>FG%</td>\n",
       "      <td>2PM</td>\n",
       "      <td>Sfty</td>\n",
       "      <td>Pts</td>\n",
       "      <td>Pos</td>\n",
       "      <td>-9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36201</td>\n",
       "      <td>Roderic Teamer</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>TeamRo00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36202</td>\n",
       "      <td>Tim Tebow</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>QB</td>\n",
       "      <td>TeboTi00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36203</td>\n",
       "      <td>Sam Tecklenburg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>TeckSa00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36204</td>\n",
       "      <td>Marvell Tell III</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DB</td>\n",
       "      <td>TellMa00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>46144.0</td>\n",
       "      <td>John Skelton</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>QB</td>\n",
       "      <td>SkelJo00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>46145.0</td>\n",
       "      <td>Alex Smith</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>QB</td>\n",
       "      <td>SmitAl03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>46146.0</td>\n",
       "      <td>Derek Anderson</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>QB</td>\n",
       "      <td>AndeDe00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>46147.0</td>\n",
       "      <td>Mike Glennon</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>QB</td>\n",
       "      <td>GlenMi00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>46148.0</td>\n",
       "      <td>Ryan Lindley</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>QB</td>\n",
       "      <td>LindRy00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9998 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5  \\\n",
       "0            Rk            Player         AV        Cmp        Att        Tgt   \n",
       "1         36201    Roderic Teamer          1          0          0          0   \n",
       "2         36202         Tim Tebow          1          6         32          1   \n",
       "3         36203   Sam Tecklenburg          1          0          0          0   \n",
       "4         36204  Marvell Tell III          1          0          0          0   \n",
       "...         ...               ...        ...        ...        ...        ...   \n",
       "9993    46144.0      John Skelton       -2.0      109.0        4.0        0.0   \n",
       "9994    46145.0        Alex Smith       -3.0       84.0       30.0        0.0   \n",
       "9995    46146.0    Derek Anderson       -4.0       81.0       10.0        0.0   \n",
       "9996    46147.0      Mike Glennon       -5.0       90.0        7.0        0.0   \n",
       "9997    46148.0      Ryan Lindley       -5.0       89.0        4.0        0.0   \n",
       "\n",
       "     Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9  ... PAT.1 Unnamed: 87   FG  \\\n",
       "0            TD         Sk        Int        Pnt  ...   XPA         XP%  FGM   \n",
       "1             0        0.0          0          0  ...     0         NaN    0   \n",
       "2             0        0.0          0          0  ...     0         NaN    0   \n",
       "3             0        0.0          0          0  ...     0         NaN    0   \n",
       "4             0        0.0          0          0  ...     0         NaN    0   \n",
       "...         ...        ...        ...        ...  ...   ...         ...  ...   \n",
       "9993        0.0        0.0        0.0        0.0  ...     0         NaN    0   \n",
       "9994        0.0        0.0        0.0        0.0  ...     0         NaN    0   \n",
       "9995        2.0        0.0        0.0        0.0  ...     0         NaN    0   \n",
       "9996        1.0        0.0        0.0        0.0  ...     0         NaN    0   \n",
       "9997        0.0        0.0        0.0        0.0  ...     0         NaN    0   \n",
       "\n",
       "     FG.1 Unnamed: 90 Unnamed: 91 Unnamed: 92 Unnamed: 93 Unnamed: 94  \\\n",
       "0     FGA         FG%         2PM        Sfty         Pts         Pos   \n",
       "1       0         NaN           0           0           0           S   \n",
       "2       0         NaN           0           0           0          QB   \n",
       "3       0         NaN           0           0           0           T   \n",
       "4       0         NaN           0           0           0          DB   \n",
       "...   ...         ...         ...         ...         ...         ...   \n",
       "9993    0         NaN         0.0         0.0         0.0          QB   \n",
       "9994    0         NaN         0.0         0.0         0.0          QB   \n",
       "9995    0         NaN         0.0         0.0        12.0          QB   \n",
       "9996    0         NaN         0.0         0.0         6.0          QB   \n",
       "9997    0         NaN         0.0         0.0         0.0          QB   \n",
       "\n",
       "     -additional  \n",
       "0          -9999  \n",
       "1       TeamRo00  \n",
       "2       TeboTi00  \n",
       "3       TeckSa00  \n",
       "4       TellMa00  \n",
       "...          ...  \n",
       "9993    SkelJo00  \n",
       "9994    SmitAl03  \n",
       "9995    AndeDe00  \n",
       "9996    GlenMi00  \n",
       "9997    LindRy00  \n",
       "\n",
       "[9998 rows x 96 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(StringIO(cleaned_csv))#[['Player', 'Player-additional', 'Wt.']]\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to NFL_all_36200+.csv.\n"
     ]
    }
   ],
   "source": [
    "# filename = 'NBA_all.csv'\n",
    "# filename = 'NBA_height.csv'\n",
    "# filename = 'NBA_weight.csv'\n",
    "filename = 'NFL_all.csv'\n",
    "# filename = 'NFL_measure.csv'\n",
    "# filename = 'MLB_B_foreign.csv'\n",
    "# filename = 'MLB_B_height.csv'\n",
    "# filename = 'MLB_B_weight.csv'\n",
    "# filename = 'MLB_P_foreign.csv'\n",
    "# filename = 'MLB_P_height.csv'\n",
    "# filename = 'MLB_P_weight.csv'\n",
    "# filename = 'MLS_all.csv'\n",
    "# filename = 'MLS_foreign.csv'\n",
    "# filename = 'NHL_foreign.csv'\n",
    "\n",
    "dataframe.to_csv(filename, index=False)\n",
    "print(f\"Data saved to {filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLS all measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('MLS_all.csv')[['Player', 'Player-additional','Ht.', 'Wt.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Player-additional</th>\n",
       "      <th>Ht.</th>\n",
       "      <th>Wt.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>Luis Arriaga</td>\n",
       "      <td>50e2ffdf</td>\n",
       "      <td>5-5</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11257</th>\n",
       "      <td>Maximo Carrizo</td>\n",
       "      <td>39b3e9d4</td>\n",
       "      <td>5-2</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>Vincent Nogueira</td>\n",
       "      <td>268c3cfc</td>\n",
       "      <td>5-6</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Player Player-additional  Ht.    Wt.\n",
       "10937      Luis Arriaga          50e2ffdf  5-5  115.0\n",
       "11257    Maximo Carrizo          39b3e9d4  5-2  119.0\n",
       "3355   Vincent Nogueira          268c3cfc  5-6  119.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort \n",
    "df.sort_values(by='Wt.', ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL Birthplaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links found: 174\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Unknown&state=FL\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=AK\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=AL\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=American%20Samoa\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=AR\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=AZ\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=CA\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=CO\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=CT\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=DC\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=DE\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=FL\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=GA\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=Guam\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=HI\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=IA\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=ID\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=IL\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=IN\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=KS\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=KY\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=LA\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=MA\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=MD\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=ME\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=MI\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=MN\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=MO\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=MS\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=MT\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=NC\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=ND\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=NE\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=NH\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=NJ\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=NM\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=NV\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=NY\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=OH\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=OK\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=OR\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=PA\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=Puerto%20Rico\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=RI\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=SC\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=SD\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=TN\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=TX\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=U.S.%20Virgin%20Islands\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=UT\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=VA\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=VT\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=WA\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=WI\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=WV\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=USA&state=WY\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Angola&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Argentina&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Armenia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Australia&state=New%20South%20Wales\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Australia&state=Victoria\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Australia&state=Western%20Australia\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Austria&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Bahamas&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Barbados&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Belgium&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Belize&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Bermuda&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Bolivia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Brazil&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Bulgaria&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Burma&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Cameroon&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Canada&state=Alberta\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Canada&state=British%20Columbia\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Canada&state=Manitoba\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Canada&state=New%20Brunswick\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Canada&state=Newfoundland%20and%20Labrador\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Canada&state=Nova%20Scotia\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Canada&state=Ontario\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Canada&state=Quebec\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Canada&state=Saskatchewan\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Chile&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Colombia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Croatia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Cuba&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Cyprus&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Czech%20Republic&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Denmark&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=DR%20Congo&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=El%20Salvador&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=England&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Estonia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Fiji&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=France&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Baden-Wurttemberg\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Bavaria\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Berlin\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Bremen\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Hamburg\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Hesse\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Lower%20Saxony\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=North%20Rhine-Westphalia\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Rhineland-Palatinate\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Saarland\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Germany&state=Saxony-Anhalt\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Ghana&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Ghana&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Greece&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Guatemala&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Guinea&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Guyana&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Haiti&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Honduras&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Hong%20Kong&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Hungary&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Iran&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Ireland&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Italy&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Ivory%20Coast&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Jamaica&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Jamaica&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Japan&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Kenya&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Kosovo&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Latvia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Lebanon&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Liberia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Libya&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Lithuania&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Marshall%20Islands&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Mexico&state=Chihuahua\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Mexico&state=Coahuila\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Mexico&state=Guanajuato\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Mexico&state=Jalisco\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Mexico&state=Mexico%20City\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Mexico&state=Michoacan\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Netherlands&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=New%20Zealand&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Nigeria&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Norway&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Okinawa&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Panama&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Panama%20Canal%20Zone&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Paraguay&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Philippines&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Poland&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Romania&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Russia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Saint%20Kitts%20and%20Nevis&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Saudi%20Arabia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Scotland&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Sierra%20Leone&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=South%20Africa&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=South%20Korea&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Spain&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Sweden&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Syria&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Taiwan&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Thailand&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Tonga&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Trinidad%20and%20Tobago&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Turkey&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Uganda&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Ukraine&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Venezuela&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Wales&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Western%20Samoa&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Western%20Samoa&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Yugoslavia&state=\n",
      "Extracted link: https://www.pro-football-reference.com/friv/birthplaces.cgi?country=Zimbabwe&state=\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Open the main page\n",
    "main_url = \"https://www.pro-football-reference.com/friv/birthplaces.htm\"\n",
    "driver.get(main_url)\n",
    "\n",
    "# Extract all links from the 'Country' and 'State' columns\n",
    "elements = driver.find_elements(By.CSS_SELECTOR, \"td[data-stat='birth_country'] a, td[data-stat='birth_state'] a\")\n",
    "urls = [element.get_attribute('href') for element in elements]\n",
    "print(f\"Total links found: {len(urls)}\")\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Extracted link: {url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_csv_data_nfl_birth(url):\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10)  # Setup wait for up to 10 seconds\n",
    "\n",
    "    # Check for \"no results\" message before proceeding\n",
    "    no_results_text = \"Sorry, there are no results for your search.\"\n",
    "    page_content = driver.page_source\n",
    "    if no_results_text in page_content:\n",
    "        print(\"No more results to fetch.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Hover over the \"Share & Export\" menu to reveal hidden menu options\n",
    "        hover_element = wait.until(EC.visibility_of_element_located(\n",
    "            (By.XPATH, '//li[@class=\"hasmore\"]/span[contains(text(),\"Share & Export\")]')))\n",
    "        ActionChains(driver).move_to_element(hover_element).perform()\n",
    "\n",
    "        # Click on the \"Get table as CSV (for Excel)\" button\n",
    "        export_button = wait.until(EC.element_to_be_clickable(\n",
    "            (By.XPATH, '//button[contains(text(),\"Get table as CSV (for Excel)\")]')))\n",
    "        export_button.click()\n",
    "\n",
    "        # Wait for the CSV data to become visible\n",
    "        wait.until(EC.visibility_of_element_located((By.ID, 'csv_birthplaces')))\n",
    "    except TimeoutException as e:\n",
    "        print(\"Timeout while waiting for elements:\", e)\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(\"Error interacting with page elements:\", e)\n",
    "        return \"\"\n",
    "\n",
    "    # Extract CSV data\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    pre_tag = soup.find('pre', id='csv_birthplaces')\n",
    "    return pre_tag.text if pre_tag else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def create_filename(base_dir, url):\n",
    "    # Parse the URL to extract query parameters\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "    # Extract 'country' and 'state' parameters, providing defaults if they are empty\n",
    "    country = query_params.get('country', ['Unknown'])[0]  # Default to 'Unknown' if missing\n",
    "    state = query_params.get('state', [''])[0]  # Default to empty string if missing\n",
    "\n",
    "    # Construct a meaningful filename\n",
    "    # If 'state' is empty, do not include it in the filename\n",
    "    if state:\n",
    "        filename = f\"{country}_{state}\"\n",
    "    else:\n",
    "        filename = f\"{country}\"\n",
    "\n",
    "    # Ensure filename does not have illegal characters and is not too long\n",
    "    filename = \"\".join([c for c in filename if c.isalnum() or c in (' ', '_', '-')]).rstrip()\n",
    "    if len(filename) > 255:  # Filename length limit for most filesystems\n",
    "        filename = filename[:255]\n",
    "\n",
    "    return os.path.join(base_dir, filename + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./NFL_birth_data/Unknown.csv\n",
      "Data saved to ./NFL_birth_data/Unknown_FL.csv\n",
      "Data saved to ./NFL_birth_data/USA.csv\n",
      "Data saved to ./NFL_birth_data/USA_AK.csv\n",
      "Data saved to ./NFL_birth_data/USA_AL.csv\n",
      "Data saved to ./NFL_birth_data/USA_American Samoa.csv\n",
      "Data saved to ./NFL_birth_data/USA_AR.csv\n",
      "Data saved to ./NFL_birth_data/USA_AZ.csv\n",
      "Data saved to ./NFL_birth_data/USA_CA.csv\n",
      "Data saved to ./NFL_birth_data/USA_CO.csv\n",
      "Data saved to ./NFL_birth_data/USA_CT.csv\n",
      "Data saved to ./NFL_birth_data/USA_DC.csv\n",
      "Data saved to ./NFL_birth_data/USA_DE.csv\n",
      "Data saved to ./NFL_birth_data/USA_FL.csv\n",
      "Data saved to ./NFL_birth_data/USA_GA.csv\n",
      "Data saved to ./NFL_birth_data/USA_Guam.csv\n",
      "Data saved to ./NFL_birth_data/USA_HI.csv\n",
      "Data saved to ./NFL_birth_data/USA_IA.csv\n",
      "Data saved to ./NFL_birth_data/USA_ID.csv\n",
      "Data saved to ./NFL_birth_data/USA_IL.csv\n",
      "Data saved to ./NFL_birth_data/USA_IN.csv\n",
      "Data saved to ./NFL_birth_data/USA_KS.csv\n",
      "Data saved to ./NFL_birth_data/USA_KY.csv\n",
      "Data saved to ./NFL_birth_data/USA_LA.csv\n",
      "Data saved to ./NFL_birth_data/USA_MA.csv\n",
      "Data saved to ./NFL_birth_data/USA_MD.csv\n",
      "Data saved to ./NFL_birth_data/USA_ME.csv\n",
      "Data saved to ./NFL_birth_data/USA_MI.csv\n",
      "Data saved to ./NFL_birth_data/USA_MN.csv\n",
      "Data saved to ./NFL_birth_data/USA_MO.csv\n",
      "Data saved to ./NFL_birth_data/USA_MS.csv\n",
      "Data saved to ./NFL_birth_data/USA_MT.csv\n",
      "Data saved to ./NFL_birth_data/USA_NC.csv\n",
      "Data saved to ./NFL_birth_data/USA_ND.csv\n",
      "Data saved to ./NFL_birth_data/USA_NE.csv\n",
      "Data saved to ./NFL_birth_data/USA_NH.csv\n",
      "Data saved to ./NFL_birth_data/USA_NJ.csv\n",
      "Data saved to ./NFL_birth_data/USA_NM.csv\n",
      "Data saved to ./NFL_birth_data/USA_NV.csv\n",
      "Data saved to ./NFL_birth_data/USA_NY.csv\n",
      "Data saved to ./NFL_birth_data/USA_OH.csv\n",
      "Data saved to ./NFL_birth_data/USA_OK.csv\n",
      "Data saved to ./NFL_birth_data/USA_OR.csv\n",
      "Data saved to ./NFL_birth_data/USA_PA.csv\n",
      "Data saved to ./NFL_birth_data/USA_Puerto Rico.csv\n",
      "Data saved to ./NFL_birth_data/USA_RI.csv\n",
      "Data saved to ./NFL_birth_data/USA_SC.csv\n",
      "Data saved to ./NFL_birth_data/USA_SD.csv\n",
      "Data saved to ./NFL_birth_data/USA_TN.csv\n",
      "Data saved to ./NFL_birth_data/USA_TX.csv\n",
      "Data saved to ./NFL_birth_data/USA_US Virgin Islands.csv\n",
      "Data saved to ./NFL_birth_data/USA_UT.csv\n",
      "Data saved to ./NFL_birth_data/USA_VA.csv\n",
      "Data saved to ./NFL_birth_data/USA_VT.csv\n",
      "Data saved to ./NFL_birth_data/USA_WA.csv\n",
      "Data saved to ./NFL_birth_data/USA_WI.csv\n",
      "Data saved to ./NFL_birth_data/USA_WV.csv\n",
      "Data saved to ./NFL_birth_data/USA_WY.csv\n",
      "Data saved to ./NFL_birth_data/Angola.csv\n",
      "Data saved to ./NFL_birth_data/Argentina.csv\n",
      "Data saved to ./NFL_birth_data/Armenia.csv\n",
      "Data saved to ./NFL_birth_data/Australia_New South Wales.csv\n",
      "Data saved to ./NFL_birth_data/Australia_Victoria.csv\n",
      "Data saved to ./NFL_birth_data/Australia_Western Australia.csv\n",
      "Data saved to ./NFL_birth_data/Austria.csv\n",
      "Data saved to ./NFL_birth_data/Bahamas.csv\n",
      "Data saved to ./NFL_birth_data/Barbados.csv\n",
      "Data saved to ./NFL_birth_data/Belgium.csv\n",
      "Data saved to ./NFL_birth_data/Belize.csv\n",
      "Data saved to ./NFL_birth_data/Bermuda.csv\n",
      "Data saved to ./NFL_birth_data/Bolivia.csv\n",
      "Data saved to ./NFL_birth_data/Brazil.csv\n",
      "Data saved to ./NFL_birth_data/Bulgaria.csv\n",
      "Data saved to ./NFL_birth_data/Burma.csv\n",
      "Data saved to ./NFL_birth_data/Cameroon.csv\n",
      "Data saved to ./NFL_birth_data/Canada_Alberta.csv\n",
      "Data saved to ./NFL_birth_data/Canada_British Columbia.csv\n",
      "Data saved to ./NFL_birth_data/Canada_Manitoba.csv\n",
      "Data saved to ./NFL_birth_data/Canada_New Brunswick.csv\n",
      "Data saved to ./NFL_birth_data/Canada_Newfoundland and Labrador.csv\n",
      "Data saved to ./NFL_birth_data/Canada_Nova Scotia.csv\n",
      "Data saved to ./NFL_birth_data/Canada_Ontario.csv\n",
      "Data saved to ./NFL_birth_data/Canada_Quebec.csv\n",
      "Data saved to ./NFL_birth_data/Canada_Saskatchewan.csv\n",
      "Data saved to ./NFL_birth_data/Chile.csv\n",
      "Data saved to ./NFL_birth_data/Colombia.csv\n",
      "Data saved to ./NFL_birth_data/Croatia.csv\n",
      "Data saved to ./NFL_birth_data/Cuba.csv\n",
      "Data saved to ./NFL_birth_data/Cyprus.csv\n",
      "Data saved to ./NFL_birth_data/Czech Republic.csv\n",
      "Data saved to ./NFL_birth_data/Denmark.csv\n",
      "Data saved to ./NFL_birth_data/DR Congo.csv\n",
      "Data saved to ./NFL_birth_data/El Salvador.csv\n",
      "Data saved to ./NFL_birth_data/England.csv\n",
      "Data saved to ./NFL_birth_data/Estonia.csv\n",
      "Data saved to ./NFL_birth_data/Fiji.csv\n",
      "Data saved to ./NFL_birth_data/France.csv\n",
      "Data saved to ./NFL_birth_data/Germany.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Baden-Wurttemberg.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Bavaria.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Berlin.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Bremen.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Hamburg.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Hesse.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Lower Saxony.csv\n",
      "Data saved to ./NFL_birth_data/Germany_North Rhine-Westphalia.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Rhineland-Palatinate.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Saarland.csv\n",
      "Data saved to ./NFL_birth_data/Germany_Saxony-Anhalt.csv\n",
      "Data saved to ./NFL_birth_data/Ghana.csv\n",
      "Data saved to ./NFL_birth_data/Ghana.csv\n",
      "Data saved to ./NFL_birth_data/Greece.csv\n",
      "Data saved to ./NFL_birth_data/Guatemala.csv\n",
      "Data saved to ./NFL_birth_data/Guinea.csv\n",
      "Data saved to ./NFL_birth_data/Guyana.csv\n",
      "Data saved to ./NFL_birth_data/Haiti.csv\n",
      "Data saved to ./NFL_birth_data/Honduras.csv\n",
      "Data saved to ./NFL_birth_data/Hong Kong.csv\n",
      "Data saved to ./NFL_birth_data/Hungary.csv\n",
      "Data saved to ./NFL_birth_data/Iran.csv\n",
      "Data saved to ./NFL_birth_data/Ireland.csv\n",
      "Data saved to ./NFL_birth_data/Italy.csv\n",
      "Data saved to ./NFL_birth_data/Ivory Coast.csv\n",
      "Data saved to ./NFL_birth_data/Jamaica.csv\n",
      "Data saved to ./NFL_birth_data/Jamaica.csv\n",
      "Data saved to ./NFL_birth_data/Japan.csv\n",
      "Data saved to ./NFL_birth_data/Kenya.csv\n",
      "Data saved to ./NFL_birth_data/Kosovo.csv\n",
      "Data saved to ./NFL_birth_data/Latvia.csv\n",
      "Data saved to ./NFL_birth_data/Lebanon.csv\n",
      "Data saved to ./NFL_birth_data/Liberia.csv\n",
      "Data saved to ./NFL_birth_data/Libya.csv\n",
      "Data saved to ./NFL_birth_data/Lithuania.csv\n",
      "Data saved to ./NFL_birth_data/Marshall Islands.csv\n",
      "Data saved to ./NFL_birth_data/Mexico_Chihuahua.csv\n",
      "Data saved to ./NFL_birth_data/Mexico_Coahuila.csv\n",
      "Data saved to ./NFL_birth_data/Mexico_Guanajuato.csv\n",
      "Data saved to ./NFL_birth_data/Mexico_Jalisco.csv\n",
      "Data saved to ./NFL_birth_data/Mexico_Mexico City.csv\n",
      "Data saved to ./NFL_birth_data/Mexico_Michoacan.csv\n",
      "Data saved to ./NFL_birth_data/Netherlands.csv\n",
      "Data saved to ./NFL_birth_data/New Zealand.csv\n",
      "Data saved to ./NFL_birth_data/Nigeria.csv\n",
      "Data saved to ./NFL_birth_data/Norway.csv\n",
      "Data saved to ./NFL_birth_data/Okinawa.csv\n",
      "Data saved to ./NFL_birth_data/Panama.csv\n",
      "Data saved to ./NFL_birth_data/Panama Canal Zone.csv\n",
      "Data saved to ./NFL_birth_data/Paraguay.csv\n",
      "Data saved to ./NFL_birth_data/Philippines.csv\n",
      "Data saved to ./NFL_birth_data/Poland.csv\n",
      "Data saved to ./NFL_birth_data/Romania.csv\n",
      "Data saved to ./NFL_birth_data/Russia.csv\n",
      "Data saved to ./NFL_birth_data/Saint Kitts and Nevis.csv\n",
      "Data saved to ./NFL_birth_data/Saudi Arabia.csv\n",
      "Data saved to ./NFL_birth_data/Scotland.csv\n",
      "Data saved to ./NFL_birth_data/Sierra Leone.csv\n",
      "Data saved to ./NFL_birth_data/South Africa.csv\n",
      "Data saved to ./NFL_birth_data/South Korea.csv\n",
      "Data saved to ./NFL_birth_data/Spain.csv\n",
      "Data saved to ./NFL_birth_data/Sweden.csv\n",
      "Data saved to ./NFL_birth_data/Syria.csv\n",
      "Data saved to ./NFL_birth_data/Taiwan.csv\n",
      "Data saved to ./NFL_birth_data/Thailand.csv\n",
      "Data saved to ./NFL_birth_data/Tonga.csv\n",
      "Data saved to ./NFL_birth_data/Trinidad and Tobago.csv\n",
      "Data saved to ./NFL_birth_data/Turkey.csv\n",
      "Data saved to ./NFL_birth_data/Uganda.csv\n",
      "Data saved to ./NFL_birth_data/Ukraine.csv\n",
      "Data saved to ./NFL_birth_data/Venezuela.csv\n",
      "Data saved to ./NFL_birth_data/Wales.csv\n",
      "Data saved to ./NFL_birth_data/Western Samoa.csv\n",
      "Data saved to ./NFL_birth_data/Western Samoa.csv\n",
      "Data saved to ./NFL_birth_data/Yugoslavia.csv\n",
      "Data saved to ./NFL_birth_data/Zimbabwe.csv\n"
     ]
    }
   ],
   "source": [
    "# Directory to store CSV files\n",
    "data_dir = \"./NFL_birth_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Fetch and save CSV data from each URL\n",
    "for url in urls:\n",
    "    csv_data = fetch_csv_data_nfl_birth(url)\n",
    "    if csv_data:\n",
    "        cleaned_csv = clean_csv_data(csv_data)\n",
    "        dataframe = pd.read_csv(StringIO(cleaned_csv))\n",
    "\n",
    "        # Save to CSV file\n",
    "        country_state = url.split('=')[-1]  # Simple way to name files uniquely\n",
    "        filename = create_filename(data_dir, url)\n",
    "        dataframe.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine all CSV files into one\n",
    "# all_csv_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "# combined_csv_path = \"./NFL_births.csv\"\n",
    "# with open(combined_csv_path, 'w', newline='') as combined_file:\n",
    "#     csv_writer = csv.writer(combined_file)\n",
    "#     for file in all_csv_files:\n",
    "#         with open(file, 'r') as f:\n",
    "#             csv_reader = csv.reader(f)\n",
    "#             for row in csv_reader:\n",
    "#                 csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been successfully combined.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import country_converter as coco\n",
    "\n",
    "# Directory where CSV files are stored\n",
    "data_dir = \"./NFL_birth_data\"\n",
    "\n",
    "# Function to parse country and state from the filename\n",
    "def parse_location_from_filename(filename):\n",
    "    # Assuming the filename format is 'Country_State.csv' or 'Country.csv'\n",
    "    parts = filename.replace('.csv', '').split('_')\n",
    "    country = parts[0]\n",
    "    state = parts[1] if len(parts) > 1 else \"\"\n",
    "    return country, state\n",
    "\n",
    "# Country converter setup\n",
    "converter = coco.CountryConverter()\n",
    "\n",
    "# Combine all CSV files into one\n",
    "all_csv_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "combined_csv_path = \"./NFL_births.csv\"\n",
    "\n",
    "with open(combined_csv_path, 'w', newline='') as combined_file:\n",
    "    csv_writer = csv.writer(combined_file)\n",
    "    \n",
    "    first_file = True  # Flag to help with writing headers\n",
    "    city_index = None  # To keep track of where the 'City' column is\n",
    "    \n",
    "    for file_path in all_csv_files:\n",
    "        country, state = parse_location_from_filename(os.path.basename(file_path))\n",
    "        # Standardize country names\n",
    "        country_aliases = {\n",
    "            \"Unknown\": \"United States\",\n",
    "            \"Wales\": \"United Kingdom\",\n",
    "            \"Scotland\": \"United Kingdom\",\n",
    "            \"Yugoslavia\": \"Serbia\",\n",
    "            \"Okinawa\": \"Japan\"\n",
    "        }\n",
    "        country = country_aliases.get(country, country)\n",
    "        country_code = converter.convert(names=country, to='ISO3')\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            for i, row in enumerate(csv_reader):\n",
    "                if i == 0:\n",
    "                    # Identify the 'City' column index\n",
    "                    city_index = row.index('City') if 'City' in row else None\n",
    "                    if first_file:\n",
    "                        # Write headers in the first file only\n",
    "                        csv_writer.writerow(row + ['Country', 'State', 'Birth Location'])\n",
    "                else:\n",
    "                    # Extract city from the row using the city_index\n",
    "                    city = row[city_index] if city_index is not None else \"\"\n",
    "                    birth_location = f\"{city} {country_code}\" if city else f\"{country_code}\"\n",
    "                    # Write the data row along with new columns\n",
    "                    csv_writer.writerow(row + [country, state, birth_location])\n",
    "                    \n",
    "        first_file = False  # After the first file, set this to False\n",
    "\n",
    "print(\"All files have been successfully combined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been successfully combined.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import country_converter as coco\n",
    "\n",
    "# Directory where CSV files are stored\n",
    "data_dir = \"./NFL_birth_data\"\n",
    "\n",
    "# Function to parse country and state from the filename\n",
    "def parse_location_from_filename(filename):\n",
    "    parts = filename.replace('.csv', '').split('_')\n",
    "    country = parts[0]\n",
    "    state = parts[1] if len(parts) > 1 else \"\"\n",
    "    return country, state\n",
    "\n",
    "# Define a dictionary mapping countries to their capitals\n",
    "country_capitals = {\n",
    "    \"BHS\": \"Nassau\",  # Bahamas\n",
    "    \"BRB\": \"Bridgetown\",  # Barbados\n",
    "    \"LBN\": \"Beirut\",  # Lebanon\n",
    "    \"DNK\": \"Copenhagen\",  # Denmark\n",
    "    \"ZWE\": \"Harar\",  # Zimbabwe\n",
    "    \"ITA\": \"Rome\",  # Italy\n",
    "    \"ROU\": \"Bucharest\",  # Romania\n",
    "    \"LTU\": \"Vilnius\",  # Lithuania\n",
    "    \"BLZ\": \"Belmopan\",  # Belize\n",
    "    \"JPN\": \"Tokyo\",  # Japan\n",
    "    \"IRL\": \"Dublin\",  # Ireland\n",
    "    \"HRV\": \"Zagreb\",  # Croatia\n",
    "    \"MEX\": \"Mexico City\",  # Mexico\n",
    "    \"DEU\": \"Berlin\",  # Germany\n",
    "    \"BGR\": \"Sofia\",  # Bulgaria\n",
    "    \"GUY\": \"Georgetown\",  # Guyana\n",
    "    \"CIV\": \"Yamoussoukro\",  # Ivory Coast\n",
    "    \"HTI\": \"Port-au-Prince\",  # Haiti\n",
    "    \"AUT\": \"Vienna\",  # Austria\n",
    "    \"UGA\": \"Kampala\",  # Uganda\n",
    "    \"LVA\": \"Riga\",  # Latvia\n",
    "    \"GRC\": \"Athens\",  # Greece\n",
    "    \"SWE\": \"Stockholm\",  # Sweden\n",
    "    \"USA\": \"Washington, D.C.\",  # United States\n",
    "    \"KOR\": \"Seoul\",  # South Korea\n",
    "    \"HKG\": \"Hong Kong\",  # Hong Kong\n",
    "    \"GBR\": \"London\",  # England\n",
    "    \"CMR\": \"Yaoundé\",  # Cameroon\n",
    "    \"NGA\": \"Abuja\",  # Nigeria\n",
    "    \"TON\": \"Nukuʻalofa\",  # Tonga\n",
    "    \"SYR\": \"Damascus\",  # Syria\n",
    "    \"AGO\": \"Luanda\",  # Angola\n",
    "    \"JAM\": \"Kingston\",  # Jamaica\n",
    "    \"POL\": \"Warsaw\",  # Poland\n",
    "    \"LBR\": \"Monrovia\",  # Liberia\n",
    "    \"RUS\": \"Moscow\",  # Russia\n",
    "    \"GHA\": \"Accra\",  # Ghana\n",
    "    \"WSM\": \"Apia\",  # Western Samoa\n",
    "    \"TWN\": \"Taipei\",  # Taiwan\n",
    "    \"MMR\": \"Naypyidaw\",  # Burma (Myanmar)\n",
    "    \"SLE\": \"Freetown\",  # Sierra Leone\n",
    "    \"SRB\": \"Belgrade\",  # Serbia\n",
    "    \"CAN\": \"Ottawa\",  # Canada\n",
    "    \"TTO\": \"Port of Spain\",  # Trinidad and Tobago\n",
    "    \"GIN\": \"Conakry\"  # Guinea\n",
    "}\n",
    "\n",
    "# Country converter setup\n",
    "converter = coco.CountryConverter()\n",
    "\n",
    "# Combine all CSV files into one\n",
    "all_csv_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "combined_csv_path = \"./NFL_births.csv\"\n",
    "\n",
    "with open(combined_csv_path, 'w', newline='') as combined_file:\n",
    "    csv_writer = csv.writer(combined_file)\n",
    "    \n",
    "    first_file = True  # Flag to help with writing headers\n",
    "    city_index = None  # To keep track of where the 'City' column is\n",
    "    \n",
    "    for file_path in all_csv_files:\n",
    "        country, state = parse_location_from_filename(os.path.basename(file_path))\n",
    "        country_aliases = {\n",
    "            \"Unknown\": \"United States\",\n",
    "            \"Wales\": \"United Kingdom\",\n",
    "            \"Scotland\": \"United Kingdom\",\n",
    "            \"Yugoslavia\": \"Serbia\",\n",
    "            \"Okinawa\": \"Japan\"\n",
    "        }\n",
    "        country = country_aliases.get(country, country)\n",
    "        country_code = converter.convert(names=country, to='ISO3')\n",
    "        capital = country_capitals.get(country_code, \"\")  # Get capital if available\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            for i, row in enumerate(csv_reader):\n",
    "                if i == 0:\n",
    "                    city_index = row.index('City') if 'City' in row else None\n",
    "                    if first_file:\n",
    "                        csv_writer.writerow(row + ['Country Code', 'Country', 'State', 'Birth Location'])\n",
    "                else:\n",
    "                    city = row[city_index] if city_index is not None and row[city_index] else capital\n",
    "                    birth_location = f\"{city} {country_code}\" if city else f\"{country_code}\"\n",
    "                    csv_writer.writerow(row + [country_code, country, state, birth_location])\n",
    "                    \n",
    "        first_file = False  # Reset flag after the first file\n",
    "\n",
    "print(\"All files have been successfully combined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NHL Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = \"https://www.nhl.com/stats/skaters?aggregate=0&report=bios&reportType=season&seasonFrom=19171918&seasonTo=20232024&gameType=3&sort=height&page=1&pageSize=100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Configuration for Chrome\n",
    "options = Options()\n",
    "options.headless = False  # Run in headless mode if you don't need a browser UI\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# URL and save directory setup\n",
    "base_url = \"https://www.nhl.com/stats/skaters\"\n",
    "save_dir = './NHL_measures/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Setup WebDriver\n",
    "service = Service(\"/opt/homebrew/Caskroom/chromedriver/125.0.6422.60/chromedriver-mac-arm64/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n0   chromedriver                        0x00000001033aa4a0 chromedriver + 4301984\n1   chromedriver                        0x00000001033a2de8 chromedriver + 4271592\n2   chromedriver                        0x0000000102fd419c chromedriver + 278940\n3   chromedriver                        0x00000001030162c4 chromedriver + 549572\n4   chromedriver                        0x000000010304ec5c chromedriver + 781404\n5   chromedriver                        0x000000010300b004 chromedriver + 503812\n6   chromedriver                        0x000000010300b9ec chromedriver + 506348\n7   chromedriver                        0x00000001033724e8 chromedriver + 4072680\n8   chromedriver                        0x0000000103376f94 chromedriver + 4091796\n9   chromedriver                        0x000000010335972c chromedriver + 3970860\n10  chromedriver                        0x000000010337787c chromedriver + 4094076\n11  chromedriver                        0x000000010334c6ac chromedriver + 3917484\n12  chromedriver                        0x0000000103394ae0 chromedriver + 4213472\n13  chromedriver                        0x0000000103394c5c chromedriver + 4213852\n14  chromedriver                        0x00000001033a29e0 chromedriver + 4270560\n15  libsystem_pthread.dylib             0x0000000195b6af94 _pthread_start + 136\n16  libsystem_pthread.dylib             0x0000000195b65d34 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?aggregate=0&report=bios&reportType=season&seasonFrom=19171918&seasonTo=20232024&gameType=3&sort=height&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&pageSize=100\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Wait for the table to be visible\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisibility_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.ReactTable.-striped\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Scroll to the download button and wait until it's clickable\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Tsinghua/BusinessDataAnalysis/ranking/data_analysis/.venv/lib/python3.11/site-packages/selenium/webdriver/support/wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n0   chromedriver                        0x00000001033aa4a0 chromedriver + 4301984\n1   chromedriver                        0x00000001033a2de8 chromedriver + 4271592\n2   chromedriver                        0x0000000102fd419c chromedriver + 278940\n3   chromedriver                        0x00000001030162c4 chromedriver + 549572\n4   chromedriver                        0x000000010304ec5c chromedriver + 781404\n5   chromedriver                        0x000000010300b004 chromedriver + 503812\n6   chromedriver                        0x000000010300b9ec chromedriver + 506348\n7   chromedriver                        0x00000001033724e8 chromedriver + 4072680\n8   chromedriver                        0x0000000103376f94 chromedriver + 4091796\n9   chromedriver                        0x000000010335972c chromedriver + 3970860\n10  chromedriver                        0x000000010337787c chromedriver + 4094076\n11  chromedriver                        0x000000010334c6ac chromedriver + 3917484\n12  chromedriver                        0x0000000103394ae0 chromedriver + 4213472\n13  chromedriver                        0x0000000103394c5c chromedriver + 4213852\n14  chromedriver                        0x00000001033a29e0 chromedriver + 4270560\n15  libsystem_pthread.dylib             0x0000000195b6af94 _pthread_start + 136\n16  libsystem_pthread.dylib             0x0000000195b65d34 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each page\n",
    "for page in range(44):  # Total of 44 pages\n",
    "    driver.get(f\"{base_url}?aggregate=0&report=bios&reportType=season&seasonFrom=19171918&seasonTo=20232024&gameType=3&sort=height&page={page}&pageSize=100\")\n",
    "    \n",
    "    # Wait for the table to be visible\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.visibility_of_element_located((By.CSS_SELECTOR, \".ReactTable.-striped\"))\n",
    "    )\n",
    "\n",
    "    # Scroll to the download button and wait until it's clickable\n",
    "    try:\n",
    "        button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \".styles__ExportIcon-sc-16o6kz0-0.gBbiwv\"))\n",
    "        )\n",
    "        # Scroll the element into view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", button)\n",
    "        time.sleep(2)  # Allow time for any possible UI changes\n",
    "\n",
    "        # Click the button\n",
    "        ActionChains(driver).move_to_element(button).click(button).perform()\n",
    "        \n",
    "        # Wait for the download to initiate\n",
    "        time.sleep(5)  # Adjust based on actual download start time\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download from page {page}: {str(e)}\")\n",
    "    print(\"Downloaded page\", page)\n",
    "    \n",
    "driver.quit()\n",
    "print(\"All files have been downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been combined into one CSV file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory containing the Excel files\n",
    "directory = './nhl_measures/'\n",
    "\n",
    "# Create a list to hold all the dataframes\n",
    "df_list = []\n",
    "\n",
    "# Iterate through all Excel files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        # Append the dataframe to the list\n",
    "        df_list.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a CSV file\n",
    "combined_df.to_csv('NHL_measure.csv', index=False)\n",
    "\n",
    "print(\"All files have been combined into one CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nhl_measures = pd.read_csv('./NHL_measure.csv')[['Player', 'Ht', 'Wt', 'Birth City', 'S/P', 'Ctry']]\n",
    "df_nhl = pd.read_csv('./NHL_foreign.csv')[['Player', 'Player-additional']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframes have been merged and saved.\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "# Define a function to normalize names\n",
    "def normalize_names(name):\n",
    "    return unidecode(name).strip()\n",
    "\n",
    "# Apply the normalization function to the 'Player' columns\n",
    "df_nhl_measures['Player'] = df_nhl_measures['Player'].apply(normalize_names)\n",
    "df_nhl['Player'] = df_nhl['Player'].apply(normalize_names)\n",
    "\n",
    "# Merge the dataframes on the 'Player' column\n",
    "merged_df = pd.merge(df_nhl_measures, df_nhl, on='Player', how='inner')\n",
    "\n",
    "# Rename columns\n",
    "merged_df.rename(columns={'Ht': 'Ht.', 'Wt': 'Wt.'}, inplace=True)\n",
    "\n",
    "# Create a new column for 'Birth Location'\n",
    "merged_df['Birth Location'] = merged_df['Birth City'] + ' ' + merged_df['Ctry']\n",
    "merged_df.drop(columns=['Birth City', 'S/P', 'Ctry'], inplace=True)\n",
    "\n",
    "\n",
    "# # Save the merged dataframe to CSV\n",
    "merged_df.to_csv('NHL_measure.csv', index=False)\n",
    "\n",
    "print(\"The dataframes have been merged and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert height from string to numeric, errors='coerce' will set '--' to NaN, then drop these\n",
    "merged_df['Ht.'] = pd.to_numeric(merged_df['Ht.'], errors='coerce')\n",
    "merged_df.dropna(subset=['Ht.'], inplace=True)\n",
    "\n",
    "# Function to convert inches to feet and inches format 'X-Y' where X is feet and Y is inches\n",
    "def inches_to_feet(inches):\n",
    "    feet = inches // 12\n",
    "    remaining_inches = inches % 12\n",
    "    return f'{feet}-{remaining_inches}'\n",
    "\n",
    "# Apply the conversion function to the height column\n",
    "merged_df['Ht.'] = merged_df['Ht.'].apply(inches_to_feet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Ht.</th>\n",
       "      <th>Wt.</th>\n",
       "      <th>Player-additional</th>\n",
       "      <th>Birth Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Evgeny Davydov</td>\n",
       "      <td>6-1</td>\n",
       "      <td>189</td>\n",
       "      <td>davydev01</td>\n",
       "      <td>Chelyabinsk RUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evgeny Davydov</td>\n",
       "      <td>6-1</td>\n",
       "      <td>189</td>\n",
       "      <td>davydev01</td>\n",
       "      <td>Chelyabinsk RUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Evgeny Davydov</td>\n",
       "      <td>6-1</td>\n",
       "      <td>189</td>\n",
       "      <td>davydev01</td>\n",
       "      <td>Chelyabinsk RUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bill Huard</td>\n",
       "      <td>6-1</td>\n",
       "      <td>215</td>\n",
       "      <td>huardbi01</td>\n",
       "      <td>Welland CAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Huard</td>\n",
       "      <td>6-1</td>\n",
       "      <td>215</td>\n",
       "      <td>huardbi01</td>\n",
       "      <td>Welland CAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23323</th>\n",
       "      <td>Gary Bergman</td>\n",
       "      <td>5-11</td>\n",
       "      <td>188</td>\n",
       "      <td>bergmga01</td>\n",
       "      <td>Kenora CAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23324</th>\n",
       "      <td>Gary Bergman</td>\n",
       "      <td>5-11</td>\n",
       "      <td>188</td>\n",
       "      <td>bergmga01</td>\n",
       "      <td>Kenora CAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23325</th>\n",
       "      <td>Gary Bergman</td>\n",
       "      <td>5-11</td>\n",
       "      <td>188</td>\n",
       "      <td>bergmga01</td>\n",
       "      <td>Kenora CAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23326</th>\n",
       "      <td>Gary Bergman</td>\n",
       "      <td>5-11</td>\n",
       "      <td>188</td>\n",
       "      <td>bergmga01</td>\n",
       "      <td>Kenora CAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23327</th>\n",
       "      <td>Gary Bergman</td>\n",
       "      <td>5-11</td>\n",
       "      <td>188</td>\n",
       "      <td>bergmga01</td>\n",
       "      <td>Kenora CAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23328 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Player   Ht.  Wt. Player-additional   Birth Location\n",
       "0      Evgeny Davydov   6-1  189         davydev01  Chelyabinsk RUS\n",
       "1      Evgeny Davydov   6-1  189         davydev01  Chelyabinsk RUS\n",
       "2      Evgeny Davydov   6-1  189         davydev01  Chelyabinsk RUS\n",
       "3          Bill Huard   6-1  215         huardbi01      Welland CAN\n",
       "4          Bill Huard   6-1  215         huardbi01      Welland CAN\n",
       "...               ...   ...  ...               ...              ...\n",
       "23323    Gary Bergman  5-11  188         bergmga01       Kenora CAN\n",
       "23324    Gary Bergman  5-11  188         bergmga01       Kenora CAN\n",
       "23325    Gary Bergman  5-11  188         bergmga01       Kenora CAN\n",
       "23326    Gary Bergman  5-11  188         bergmga01       Kenora CAN\n",
       "23327    Gary Bergman  5-11  188         bergmga01       Kenora CAN\n",
       "\n",
       "[23328 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the merged dataframe to CSV\n",
    "merged_df.to_csv('NHL_measure.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### checking measure column dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Player\n",
      "  str: Wayne Gretzky\n",
      "Column: Player-additional\n",
      "  str: gretzwa01\n",
      "Column: Birth Location\n",
      "  str: Brantford ON\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('NHL_foreign.csv')[['Player', 'Player-additional', 'Birth Location']]\n",
    "\n",
    "# Initialize a dictionary to store examples\n",
    "dtype_examples = {}\n",
    "\n",
    "# Function to find examples for each dtype in a column\n",
    "def find_dtype_examples(series):\n",
    "    examples = {}\n",
    "    for item in series.dropna():  # Drop NaN to avoid type confusion\n",
    "        item_type = type(item).__name__\n",
    "        if item_type not in examples:\n",
    "            examples[item_type] = item\n",
    "    return examples\n",
    "\n",
    "# Apply the function to each column\n",
    "for column in df.columns:\n",
    "    dtype_examples[column] = find_dtype_examples(df[column])\n",
    "\n",
    "# Output the results\n",
    "for column, examples in dtype_examples.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    for dtype, example in examples.items():\n",
    "        print(f\"  {dtype}: {example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining MLB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ordered_set import OrderedSet\n",
    "import pandas as pd\n",
    "\n",
    "def adaptive_merge(dataframes):\n",
    "    \n",
    "  merged_df = pd.DataFrame(columns=['Player', 'Player-additional', 'Season'])\n",
    "  broadcast_columns = ['Player', 'Birth Location', 'Ht.', 'Wt.', 'BMI', 'Pos', 'Country', 'State']\n",
    "  for df in dataframes:\n",
    "    if 'Season' in df.columns:\n",
    "        merge_keys = ['Player-additional', 'Season']\n",
    "    else:\n",
    "        merge_keys = ['Player-additional']\n",
    "        # Treat all columns as broadcast columns if the data does not contain a 'Season' column\n",
    "        # broadcast_columns.extend([col for col in df.columns if col not in ['Player-additional', 'Season']])\n",
    "\n",
    "    # Performing the merge with suffixes for overlapping column names\n",
    "    merged_df = pd.merge(merged_df, df, on=merge_keys, how='outer', suffixes=('_x', '_y'))\n",
    "\n",
    "    final_columns = OrderedSet()\n",
    "    for column in merged_df.columns:\n",
    "      if \"_\" in column:\n",
    "        col, ind = column.rsplit(\"_\", 1)\n",
    "      else:\n",
    "        col, ind = column, \"\"\n",
    "        \n",
    "      final_columns.add(col)\n",
    "      \n",
    "      # Filter out unique columns\n",
    "      if ind != \"x\":\n",
    "        continue\n",
    "\n",
    "      # Merge all duplicate columns: '_x' columns with their '_y' columns\n",
    "      merged_df[col] = merged_df[col + \"_x\"].combine_first(merged_df[col + \"_y\"])\n",
    "\n",
    "    # Remove duplicate columns from final df\n",
    "    merged_df = merged_df[list(final_columns)]\n",
    "\n",
    "  # Fill in missing values by duplicating values for all seasons\n",
    "  for broadcast_col in broadcast_columns:\n",
    "    if broadcast_col in merged_df.columns:\n",
    "      merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
    "\n",
    "  return merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:41: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:41: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:41: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:41: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n"
     ]
    }
   ],
   "source": [
    "files = [\"MLB_B_height.csv\", \"MLB_P_height.csv\", \"MLB_B_weight.csv\", \"MLB_P_weight.csv\"]\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "merged_df = adaptive_merge(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column 'Season' from the merged dataframe\n",
    "merged_df.drop(columns=['Season'], inplace=True)\n",
    "\n",
    "# Merge all duplicate rows\n",
    "merged_df = merged_df.drop_duplicates(subset=['Player-additional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('MLB_measure.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:33: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_df[col] = merged_df[col + \"_x\"].combine_first(merged_df[col + \"_y\"])\n",
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:41: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:41: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:41: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:41: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
      "/var/folders/p8/3975cv191_3dz5xbyjd745y80000gn/T/ipykernel_5485/2590450821.py:41: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n"
     ]
    }
   ],
   "source": [
    "files = [\"MLB_B_foreign.csv\", \"MLB_P_foreign.csv\"]\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "merged_df = adaptive_merge(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Player-additional</th>\n",
       "      <th>Season</th>\n",
       "      <th>Rk</th>\n",
       "      <th>WAR</th>\n",
       "      <th>Age</th>\n",
       "      <th>Team</th>\n",
       "      <th>Lg</th>\n",
       "      <th>G</th>\n",
       "      <th>PA</th>\n",
       "      <th>...</th>\n",
       "      <th>WP</th>\n",
       "      <th>BF</th>\n",
       "      <th>ERA+</th>\n",
       "      <th>FIP</th>\n",
       "      <th>WHIP</th>\n",
       "      <th>H9</th>\n",
       "      <th>HR9</th>\n",
       "      <th>BB9</th>\n",
       "      <th>SO9</th>\n",
       "      <th>SO/BB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fernando Abad</td>\n",
       "      <td>abadfe01</td>\n",
       "      <td>2010</td>\n",
       "      <td>8896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>NL</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fernando Abad</td>\n",
       "      <td>abadfe01</td>\n",
       "      <td>2011</td>\n",
       "      <td>6790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>NL</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6.33</td>\n",
       "      <td>1.881</td>\n",
       "      <td>12.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fernando Abad</td>\n",
       "      <td>abadfe01</td>\n",
       "      <td>2012</td>\n",
       "      <td>9722</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>NL</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>4.57</td>\n",
       "      <td>1.652</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fernando Abad</td>\n",
       "      <td>abadfe01</td>\n",
       "      <td>2013</td>\n",
       "      <td>6791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>WSN</td>\n",
       "      <td>NL</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.381</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fernando Abad</td>\n",
       "      <td>abadfe01</td>\n",
       "      <td>2014</td>\n",
       "      <td>6792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>OAK</td>\n",
       "      <td>AL</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.855</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15841</th>\n",
       "      <td>Jordan Zimmerman</td>\n",
       "      <td>zimmejo01</td>\n",
       "      <td>1999</td>\n",
       "      <td>8523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SEA</td>\n",
       "      <td>AL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.26</td>\n",
       "      <td>2.250</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15842</th>\n",
       "      <td>Julio Zuleta</td>\n",
       "      <td>zuletju01</td>\n",
       "      <td>2000</td>\n",
       "      <td>5141</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>CHC</td>\n",
       "      <td>NL</td>\n",
       "      <td>30.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15843</th>\n",
       "      <td>Julio Zuleta</td>\n",
       "      <td>zuletju01</td>\n",
       "      <td>2001</td>\n",
       "      <td>12470</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>26.0</td>\n",
       "      <td>CHC</td>\n",
       "      <td>NL</td>\n",
       "      <td>49.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15844</th>\n",
       "      <td>Guillo Zuñiga</td>\n",
       "      <td>zuniggu01</td>\n",
       "      <td>2023</td>\n",
       "      <td>15845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>STL</td>\n",
       "      <td>NL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15845</th>\n",
       "      <td>Guillo Zuñiga</td>\n",
       "      <td>zuniggu01</td>\n",
       "      <td>2024</td>\n",
       "      <td>15846</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>LAA</td>\n",
       "      <td>AL</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.34</td>\n",
       "      <td>1.600</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15846 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Player Player-additional  Season     Rk  WAR   Age Team  Lg  \\\n",
       "0         Fernando Abad          abadfe01    2010   8896  0.0  24.0  HOU  NL   \n",
       "1         Fernando Abad          abadfe01    2011   6790  0.0  25.0  HOU  NL   \n",
       "2         Fernando Abad          abadfe01    2012   9722 -0.1  26.0  HOU  NL   \n",
       "3         Fernando Abad          abadfe01    2013   6791  0.0  27.0  WSN  NL   \n",
       "4         Fernando Abad          abadfe01    2014   6792  0.0  28.0  OAK  AL   \n",
       "...                 ...               ...     ...    ...  ...   ...  ...  ..   \n",
       "15841  Jordan Zimmerman         zimmejo01    1999   8523  0.0  24.0  SEA  AL   \n",
       "15842      Julio Zuleta         zuletju01    2000   5141  0.2  25.0  CHC  NL   \n",
       "15843      Julio Zuleta         zuletju01    2001  12470 -0.4  26.0  CHC  NL   \n",
       "15844     Guillo Zuñiga         zuniggu01    2023  15845  0.0  24.0  STL  NL   \n",
       "15845     Guillo Zuñiga         zuniggu01    2024  15846 -0.1  25.0  LAA  AL   \n",
       "\n",
       "          G     PA  ...   WP     BF   ERA+   FIP   WHIP    H9  HR9  BB9   SO9  \\\n",
       "0      22.0    1.0  ...  0.0   76.0  142.0  4.66  1.000   6.6  1.4  2.4   5.7   \n",
       "1      28.0    0.0  ...  0.0   99.0   53.0  6.33  1.881  12.8  2.3  4.1   6.9   \n",
       "2      36.0    7.0  ...  4.0  208.0   80.0  4.57  1.652  11.2  1.2  3.7   7.4   \n",
       "3      34.0    0.0  ...  0.0  166.0  114.0  3.26  1.381  10.0  0.7  2.4   7.6   \n",
       "4       7.0    0.0  ...  0.0  216.0  238.0  3.25  0.855   5.3  0.6  2.4   8.0   \n",
       "...     ...    ...  ...  ...    ...    ...   ...    ...   ...  ...  ...   ...   \n",
       "15841   2.0    0.0  ...  1.0   41.0   66.0  4.26  2.250  15.8  0.0  4.5   3.4   \n",
       "15842  30.0   73.0  ...  NaN    NaN    NaN   NaN    NaN   NaN  NaN  NaN   NaN   \n",
       "15843  49.0  118.0  ...  NaN    NaN    NaN   NaN    NaN   NaN  NaN  NaN   NaN   \n",
       "15844   2.0    NaN  ...  0.0    8.0  113.0 -0.75  1.000   9.0  0.0  0.0  18.0   \n",
       "15845   4.0    NaN  ...  1.0   23.0   61.0  7.34  1.600   7.2  1.8  7.2   3.6   \n",
       "\n",
       "       SO/BB  \n",
       "0       2.40  \n",
       "1       1.67  \n",
       "2       2.00  \n",
       "3       3.20  \n",
       "4       3.40  \n",
       "...      ...  \n",
       "15841   0.75  \n",
       "15842    NaN  \n",
       "15843    NaN  \n",
       "15844    NaN  \n",
       "15845   0.50  \n",
       "\n",
       "[15846 rows x 66 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('MLB_foreign.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
