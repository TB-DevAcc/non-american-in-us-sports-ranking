{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import country_converter as coco\n",
    "from typing import Optional, Tuple\n",
    "import logging\n",
    "import pycountry\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
    "import warnings\n",
    "from babel.languages import get_territory_language_info\n",
    "\n",
    "# Suppress only FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "coco_logger = coco.logging.getLogger()\n",
    "coco_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Using geopy to get geographical coordinates\n",
    "geolocator = Nominatim(user_agent=\"MyPersonalGeocoder/1.0 (myemail@example.com)\", timeout=10)\n",
    "\n",
    "# Dictionary mapping file names to their respective processing functions\n",
    "files = {\n",
    "  \"NBA\": [\"NBA_foreign.csv\"],\n",
    "  \"NFL\": [\"NFL_all.csv\", \"NFL_measure.csv\", \"NFL_births.csv\"],\n",
    "  \"MLB\": [\"MLB_B_foreign.csv\", \"MLB_P_foreign.csv\"],\n",
    "  \"NHL\": [\"NHL_foreign.csv\"],\n",
    "  \"MLS\": [\"MLS_foreign.csv\"],\n",
    "}\n",
    "league_files = {}\n",
    "\n",
    "# Global list of final column names\n",
    "final_columns = ['Player', 'Player-additional', 'League', 'Home Continent', 'Home Country', 'Home City', 'Overall Value', 'Offensive Performance', 'Defensive Performance', 'Measurables', 'Migration Difficulty']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_value(row, column_name: str, type_=str):\n",
    "    try:\n",
    "        value = row[column_name]\n",
    "        if not isinstance(value, type_):\n",
    "            return np.nan\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError accessing row[{column_name}]: {str(e)}, with row {row}\")\n",
    "        return np.nan\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for the CSV files\n",
    "country_df_path = 'countries.csv'\n",
    "location_df_path = 'locations.csv'\n",
    "distance_df_path = 'distances.csv'\n",
    "\n",
    "# Load or initialize the country dataframe\n",
    "if os.path.exists(country_df_path):\n",
    "    country_df = pd.read_csv(country_df_path)\n",
    "else:\n",
    "    country_df = pd.DataFrame(columns=['code', 'country_name', 'continent', 'language', 'language_tier'])\n",
    "\n",
    "# Load or initialize the location dataframe\n",
    "if os.path.exists(location_df_path):\n",
    "    location_df = pd.read_csv(location_df_path)\n",
    "else:\n",
    "    location_df = pd.DataFrame(columns=['city', 'country', 'latitude', 'longitude'])\n",
    "\n",
    "# Load or initialize the distance dataframe\n",
    "if os.path.exists(distance_df_path):\n",
    "    distance_df = pd.read_csv(distance_df_path)\n",
    "else:\n",
    "    distance_df = pd.DataFrame(columns=['city_coords', 'sf_distance', 'ny_distance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_name(code: str):\n",
    "    global country_df\n",
    "    \n",
    "    match = country_df[country_df['code'] == code]\n",
    "    if not match.empty and not pd.isna(match['country_name'].iloc[0]):\n",
    "        return match['country_name'].iloc[0]\n",
    "    \n",
    "    # Call coco if no data is found or if the country name is NaN\n",
    "    country_name = coco.convert(names=code, to='name_short')\n",
    "    if match.empty:\n",
    "        new_row = pd.DataFrame({'code': [code], 'country_name': [country_name], 'continent': [pd.NA]})\n",
    "        country_df = pd.concat([country_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        country_df.loc[country_df['code'] == code, 'country_name'] = country_name\n",
    "    return country_name\n",
    "\n",
    "def get_continent(code: str):\n",
    "    global country_df\n",
    "    \n",
    "    match = country_df[country_df['code'] == code]\n",
    "    if not match.empty and 'continent' in match.columns and not pd.isna(match['continent'].iloc[0]):\n",
    "        return match['continent'].iloc[0]\n",
    "    \n",
    "    # Call coco if no data is found or continent is NaN\n",
    "    continent = coco.convert(names=code, to='Continent_7')\n",
    "    if match.empty:\n",
    "        new_row = pd.DataFrame({'code': [code], 'country_name': [pd.NA], 'continent': [continent]})\n",
    "        country_df = pd.concat([country_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        country_df.loc[country_df['code'] == code, 'continent'] = continent\n",
    "    return continent\n",
    "\n",
    "def lookup_location(city: str, country: str) -> Tuple[float, float]:\n",
    "    global location_df\n",
    "\n",
    "    match = location_df[(location_df['city'] == city) & (location_df['country'] == country)]\n",
    "    if not match.empty:\n",
    "        return match[['latitude', 'longitude']].iloc[0].tolist()\n",
    "    \n",
    "    try:\n",
    "        location = geolocator.geocode(f\"{city}, {country}\")\n",
    "        if location:\n",
    "            new_row = pd.DataFrame({'city': [city], 'country': [country], 'latitude': [location.latitude], 'longitude': [location.longitude]})\n",
    "            location_df = pd.concat([location_df, new_row], ignore_index=True)\n",
    "            return (location.latitude, location.longitude)\n",
    "    except (GeocoderTimedOut, GeocoderUnavailable) as e:\n",
    "        print(f\"Geocoding error due to timeout or unavailability: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_sf_distance(city_coords: Tuple[float, float], san_francisco_coords: Tuple[float, float] = (37.7749, -122.4194)):\n",
    "    global distance_df\n",
    "\n",
    "    # Convert city coordinates to a string format for easy comparison\n",
    "    city_coords_str = f\"{city_coords[0]},{city_coords[1]}\"\n",
    "\n",
    "    # Check if the distance already exists in the dataframe\n",
    "    match = distance_df[distance_df['city_coords'] == city_coords_str]\n",
    "    if not match.empty and not pd.isna(match['sf_distance'].iloc[0]):\n",
    "        return match['sf_distance'].iloc[0]\n",
    "    \n",
    "    # Calculate distance if no match is found\n",
    "    distance = geodesic(city_coords, san_francisco_coords).kilometers\n",
    "    if match.empty:\n",
    "        # Add new row if no entry exists for these city coordinates\n",
    "        new_row = pd.DataFrame({'city_coords': [city_coords_str], 'sf_distance': [distance], 'ny_distance': [pd.NA]})\n",
    "        distance_df = pd.concat([distance_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        # Update existing row with SF distance if it was NaN\n",
    "        distance_df.loc[distance_df['city_coords'] == city_coords_str, 'sf_distance'] = distance\n",
    "    return distance\n",
    "\n",
    "def get_ny_distance(city_coords: Tuple[float, float], new_york_coords: Tuple[float, float] = (40.7128, -74.0060)):\n",
    "    global distance_df\n",
    "\n",
    "    city_coords_str = f\"{city_coords[0]},{city_coords[1]}\"\n",
    "\n",
    "    match = distance_df[distance_df['city_coords'] == city_coords_str]\n",
    "    if not match.empty and not pd.isna(match['ny_distance'].iloc[0]):\n",
    "        return match['ny_distance'].iloc[0]\n",
    "    \n",
    "    # Calculate distance if no match is found\n",
    "    distance = geodesic(city_coords, new_york_coords).kilometers\n",
    "    if match.empty:\n",
    "        # Add new row if no entry exists for these city coordinates\n",
    "        new_row = pd.DataFrame({'city_coords': [city_coords_str], 'sf_distance': [pd.NA], 'ny_distance': [distance]})\n",
    "        distance_df = pd.concat([distance_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        # Update existing row with NY distance if it was NaN\n",
    "        distance_df.loc[distance_df['city_coords'] == city_coords_str, 'ny_distance'] = distance\n",
    "    return distance\n",
    "\n",
    "def get_distance(city, country):\n",
    "    city_coords = lookup_location(city, country)\n",
    "    if city_coords:\n",
    "        # Earth circumference assumptions\n",
    "        max_distance = 20037.5  # Half the Earth's circumference in km\n",
    "\n",
    "        distance_to_sf = get_sf_distance(city_coords)\n",
    "        distance_to_ny = get_ny_distance(city_coords)\n",
    "        distance = min(distance_to_sf, distance_to_ny)\n",
    "        distance = distance / max_distance # normalize to [0, 1]\n",
    "    else:\n",
    "        distance = 0\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language_tiers = {\n",
    "#     \"English\": 0,\n",
    "#     \"Afrikaans\": 1, \"Danish\": 1, \"Dutch\": 1, \"French\": 1, \"Italian\": 1, \n",
    "#     \"Norwegian\": 1, \"Portuguese\": 1, \"Romanian\": 1, \"Spanish\": 1, \"Swedish\": 1,\n",
    "#     \"German\": 2,\n",
    "#     \"Indonesian\": 3, \"Malaysian\": 3, \"Swahili\": 3,\n",
    "#     \"Albanian\": 4, \"Amharic\": 4, \"Armenian\": 4, \"Azerbaijani\": 4, \"Bengali\": 4, \n",
    "#     \"Bosnian\": 4, \"Bulgarian\": 4, \"Burmese\": 4, \"Croatian\": 4, \"Czech\": 4,\n",
    "#     \"Estonian\": 4, \"Finnish\": 4, \"Georgian\": 4, \"Greek\": 4, \"Hebrew\": 4, \n",
    "#     \"Hindi\": 4, \"Hungarian\": 4, \"Icelandic\": 4, \"Khmer\": 4, \"Lao\": 4, \n",
    "#     \"Latvian\": 4, \"Lithuanian\": 4, \"Macedonian\": 4, \"Mongolian\": 4, \n",
    "#     \"Nepali\": 4, \"Pashto\": 4, \"Persian\": 4, \"Polish\": 4, \"Russian\": 4, \n",
    "#     \"Serbian\": 4, \"Sinhala\": 4, \"Slovak\": 4, \"Slovenian\": 4, \"Tagalog\": 4, \n",
    "#     \"Thai\": 4, \"Turkish\": 4, \"Ukrainian\": 4, \"Urdu\": 4, \"Uzbek\": 4, \n",
    "#     \"Vietnamese\": 4, \"Xhosa\": 4, \"Zulu\": 4,\n",
    "#     \"Arabic\": 5, \"Cantonese\": 5, \"Mandarin\": 5, \"Japanese\": 5, \"Korean\": 5\n",
    "# }\n",
    "\n",
    "language_tiers = {\n",
    "    \"en\": 0,  # English\n",
    "    \"af\": 1,  # Afrikaans\n",
    "    \"da\": 1,  # Danish\n",
    "    \"nl\": 1,  # Dutch\n",
    "    \"fr\": 1,  # French\n",
    "    \"it\": 1,  # Italian\n",
    "    \"no\": 1,  # Norwegian\n",
    "    \"pt\": 1,  # Portuguese\n",
    "    \"ro\": 1,  # Romanian\n",
    "    \"es\": 1,  # Spanish\n",
    "    \"sv\": 1,  # Swedish\n",
    "    \"de\": 2,  # German\n",
    "    \"id\": 3,  # Indonesian\n",
    "    \"ms\": 3,  # Malaysian\n",
    "    \"sw\": 3,  # Swahili\n",
    "    \"sq\": 4,  # Albanian\n",
    "    \"am\": 4,  # Amharic\n",
    "    \"hy\": 4,  # Armenian\n",
    "    \"az\": 4,  # Azerbaijani\n",
    "    \"bn\": 4,  # Bengali\n",
    "    \"bs\": 4,  # Bosnian\n",
    "    \"bg\": 4,  # Bulgarian\n",
    "    \"my\": 4,  # Burmese\n",
    "    \"hr\": 4,  # Croatian\n",
    "    \"cs\": 4,  # Czech\n",
    "    \"et\": 4,  # Estonian\n",
    "    \"fi\": 4,  # Finnish\n",
    "    \"ka\": 4,  # Georgian\n",
    "    \"el\": 4,  # Greek\n",
    "    \"he\": 4,  # Hebrew\n",
    "    \"hi\": 4,  # Hindi\n",
    "    \"hu\": 4,  # Hungarian\n",
    "    \"is\": 4,  # Icelandic\n",
    "    \"km\": 4,  # Khmer\n",
    "    \"lo\": 4,  # Lao\n",
    "    \"lv\": 4,  # Latvian\n",
    "    \"lt\": 4,  # Lithuanian\n",
    "    \"mk\": 4,  # Macedonian\n",
    "    \"mn\": 4,  # Mongolian\n",
    "    \"ne\": 4,  # Nepali\n",
    "    \"ps\": 4,  # Pashto\n",
    "    \"fa\": 4,  # Persian\n",
    "    \"pl\": 4,  # Polish\n",
    "    \"ru\": 4,  # Russian\n",
    "    \"sr\": 4,  # Serbian\n",
    "    \"si\": 4,  # Sinhala\n",
    "    \"sk\": 4,  # Slovak\n",
    "    \"sl\": 4,  # Slovenian\n",
    "    \"tl\": 4,  # Tagalog (Filipino)\n",
    "    \"th\": 4,  # Thai\n",
    "    \"tr\": 4,  # Turkish\n",
    "    \"uk\": 4,  # Ukrainian\n",
    "    \"ur\": 4,  # Urdu\n",
    "    \"uz\": 4,  # Uzbek\n",
    "    \"vi\": 4,  # Vietnamese\n",
    "    \"xh\": 4,  # Xhosa\n",
    "    \"zu\": 4,  # Zulu\n",
    "    \"ar\": 5,  # Arabic\n",
    "    \"yue\": 5, # Cantonese\n",
    "    \"zh\": 5,  # Mandarin\n",
    "    \"ja\": 5,  # Japanese\n",
    "    \"ko\": 5   # Korean\n",
    "}\n",
    "\n",
    "def get_lowest_tier_language(code: str):\n",
    "    global country_df\n",
    "    \n",
    "    # Convert the provided code to ISO alpha-2 for consistency in DataFrame\n",
    "    alpha_2_code = coco.convert(names=code, to='ISO2')\n",
    "    \n",
    "    # First, check if the data already exists in the DataFrame\n",
    "    match = country_df[country_df['code'] == code]\n",
    "    if not match.empty and not pd.isna(match['language'].iloc[0]) and not pd.isna(match['language_tier'].iloc[0]):\n",
    "        return match['language'].iloc[0], match['language_tier'].iloc[0]\n",
    "\n",
    "    # Fetch all languages spoken in the territory along with their details\n",
    "    languages_info = get_territory_language_info(alpha_2_code)\n",
    "    \n",
    "    # Filter to include only official languages with significant population percentage\n",
    "    official_languages = {\n",
    "        lang: info for lang, info in languages_info.items()\n",
    "        if info.get('official_status') and info.get('population_percent', 0) >= 30\n",
    "    }\n",
    "\n",
    "    if not official_languages:\n",
    "        return None, None\n",
    "\n",
    "    # Assuming language_tiers dictionary maps language codes to their respective tiers\n",
    "    default_tier = 5\n",
    "    lowest_tier_language = None\n",
    "    lowest_tier = default_tier\n",
    "\n",
    "    # Determine the language with the lowest tier\n",
    "    for language, _ in official_languages.items():\n",
    "        language_tier = language_tiers.get(language, default_tier)\n",
    "        if language_tier <= lowest_tier:\n",
    "            lowest_tier = language_tier\n",
    "            lowest_tier_language = language\n",
    "\n",
    "    # Update the DataFrame with new data if not found\n",
    "    if match.empty:\n",
    "        new_row = pd.DataFrame({\n",
    "            'code': [code], \n",
    "            'language': [lowest_tier_language], \n",
    "            'language_tier': [lowest_tier]\n",
    "        })\n",
    "        country_df = pd.concat([country_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        country_df.loc[country_df['code'] == code, 'language'] = lowest_tier_language\n",
    "        country_df.loc[country_df['code'] == code, 'language_tier'] = lowest_tier\n",
    "\n",
    "    return lowest_tier_language, lowest_tier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_geo_dataframes():\n",
    "    country_df.to_csv(country_df_path, index=False)\n",
    "    location_df.to_csv(location_df_path, index=False)\n",
    "    distance_df.to_csv(distance_df_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "def extract_player_generic(row):\n",
    "    return get_row_value(row, \"Player\", type_=str)\n",
    "    \n",
    "def extract_player_additional_generic(row):\n",
    "    return get_row_value(row, \"Player-additional\", type_=str)\n",
    "\n",
    "\n",
    "# Define a dictionary for U.S. and Canadian state codes to their country ISO codes\n",
    "state_to_country = {\n",
    "    # U.S. state codes (abbreviated for brevity)\n",
    "    \"AL\": \"USA\", \"AK\": \"USA\", \"AZ\": \"USA\", \"AR\": \"USA\", \"CA\": \"USA\",\n",
    "    \"CO\": \"USA\", \"CT\": \"USA\", \"DE\": \"USA\", \"FL\": \"USA\", \"GA\": \"USA\",\n",
    "    \"HI\": \"USA\", \"ID\": \"USA\", \"IL\": \"USA\", \"IN\": \"USA\", \"IA\": \"USA\",\n",
    "    \"KS\": \"USA\", \"KY\": \"USA\", \"LA\": \"USA\", \"ME\": \"USA\", \"MD\": \"USA\",\n",
    "    \"MA\": \"USA\", \"MI\": \"USA\", \"MN\": \"USA\", \"MS\": \"USA\", \"MO\": \"USA\",\n",
    "    \"MT\": \"USA\", \"NE\": \"USA\", \"NV\": \"USA\", \"NH\": \"USA\", \"NJ\": \"USA\",\n",
    "    \"NM\": \"USA\", \"NY\": \"USA\", \"NC\": \"USA\", \"ND\": \"USA\", \"OH\": \"USA\",\n",
    "    \"OK\": \"USA\", \"OR\": \"USA\", \"PA\": \"USA\", \"RI\": \"USA\", \"SC\": \"USA\",\n",
    "    \"SD\": \"USA\", \"TN\": \"USA\", \"TX\": \"USA\", \"UT\": \"USA\", \"VT\": \"USA\",\n",
    "    \"VA\": \"USA\", \"WA\": \"USA\", \"WV\": \"USA\", \"WI\": \"USA\", \"WY\": \"USA\",\n",
    "    # Canadian province codes\n",
    "    \"AB\": \"CAN\", \"BC\": \"CAN\", \"MB\": \"CAN\", \"NB\": \"CAN\", \"NL\": \"CAN\",\n",
    "    \"NS\": \"CAN\", \"ON\": \"CAN\", \"PE\": \"CAN\", \"QC\": \"CAN\", \"SK\": \"CAN\",\n",
    "    \"NT\": \"CAN\", \"NU\": \"CAN\", \"YT\": \"CAN\",\n",
    "    # Other codes\n",
    "    \"SUN\": \"RUS\", \"CRK\": \"CZE\", \"ENG\": \"GBR\", \"CSK\": \"SVK\", \"DDR\": \"DEU\", \"YUG\": \"SRB\"\n",
    "}\n",
    "\n",
    "def extract_location_generic(row) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extracts values for columns 'Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"\n",
    "    \"\"\"\n",
    "    code = get_row_value(row, 'Birth Location', type_=str)\n",
    "    if pd.isna(code) or \"\":\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "    # Split from the back and only split once\n",
    "    city, code = code.rsplit(' ', 1)\n",
    "\n",
    "    # First check against state codes, then use country_converter if needed\n",
    "    code = state_to_country.get(code, code)\n",
    "\n",
    "    # Find country names\n",
    "    country = get_country_name(code)\n",
    "    if country == \"not_found\":\n",
    "        print(f\"Country code {code} not found in row {row}\")\n",
    "        country = np.nan\n",
    "\n",
    "    # Convert code to continent\n",
    "    continent = get_continent(code)\n",
    "    if continent == \"not_found\":\n",
    "        print(f\"Continent code {code} not found in row {row}\")\n",
    "        continent = np.nan\n",
    "\n",
    "    language, language_tier = get_lowest_tier_language(code)\n",
    "    if not language or not language_tier:\n",
    "        language = np.nan\n",
    "        language_tier = 0\n",
    "\n",
    "    language_tier = language_tier / 5 # normalize to [0, 1]\n",
    "\n",
    "    distance = get_distance(city, country)\n",
    "\n",
    "    migration = int(language_tier * 75 + distance * 25)\n",
    "\n",
    "    return (continent, country, city, migration)\n",
    "\n",
    "def extract_migration_difficulty_generic(row):\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Value\n",
    "\n",
    "# Constants for maximum values used in normalization\n",
    "SCORE_WS = (-3, 26) # Win Shares (Woody Sauldsberry -2.8 1960-61) - (Kareem Abdul-Jabbar 25.4 1971-72)\n",
    "SCORE_AV = (-6, 26) # Approximate Value (Dan Pastorini -6 1981) - (LaDainian Tomlinson 26 2006)\n",
    "SCORE_WAR = (-6, 21) # Wins Above Replacement (Jersey Bakley -5.3 1884) - (Pud Galvin 20.5 1884)\n",
    "PLUS_MINUS_MLS = (-38, 48) # Plus-Minus (Mathieu Deplagne -38 2019) - (Carlos Vela 48 2019)\n",
    "SCORE_PS = (-2.4, 23) # Point Shares (Ken Baumgartner -2.4 1997-98) - (Bobby Orr 22.8 1970-71)\n",
    "\n",
    "def extract_overall_value_nba(row):\n",
    "    win_shares = get_row_value(row, 'WS', float)\n",
    "    all_star_appearances = get_row_value(row, 'AS', float)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    win_shares = 0 if np.isnan(win_shares) else win_shares\n",
    "    all_star_appearances = 0 if np.isnan(all_star_appearances) else all_star_appearances\n",
    "\n",
    "    # Normalize with new range\n",
    "    normalized_win_shares = (win_shares - SCORE_WS[0]) / (SCORE_WS[1] - SCORE_WS[0]) * 50\n",
    "    score_all_star = all_star_appearances * 50\n",
    "\n",
    "    total_score = int(normalized_win_shares + score_all_star)\n",
    "    return total_score\n",
    "\n",
    "def extract_overall_value_nfl(row):\n",
    "    av = get_row_value(row, 'AV', float)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    av = 0 if np.isnan(av) else av\n",
    "\n",
    "    # Normalize with new range\n",
    "    normalized_av = (av - SCORE_AV[0]) / (SCORE_AV[1] - SCORE_AV[0]) * 100\n",
    "    return int(normalized_av)\n",
    "\n",
    "def extract_overall_value_mlb(row):\n",
    "    war = get_row_value(row, 'WAR', float)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    war = 0 if np.isnan(war) else war\n",
    "\n",
    "    # Normalize with new range\n",
    "    normalized_war = (war - SCORE_WAR[0]) / (SCORE_WAR[1] - SCORE_WAR[0]) * 100\n",
    "    return int(normalized_war)\n",
    "\n",
    "def extract_overall_value_mls(row):\n",
    "    plus_minus = get_row_value(row, '+/-', int)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    plus_minus = 0 if np.isnan(plus_minus) else plus_minus\n",
    "\n",
    "    # Normalize with new range\n",
    "    normalized_plus_minus = (plus_minus - PLUS_MINUS_MLS[0]) / (PLUS_MINUS_MLS[1] - PLUS_MINUS_MLS[0]) * 100\n",
    "    return int(normalized_plus_minus)\n",
    "\n",
    "def extract_overall_value_nhl(row):\n",
    "    ps = get_row_value(row, 'PS', float)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    ps = 0 if np.isnan(ps) else ps\n",
    "\n",
    "    # Normalize with new range\n",
    "    normalized_ps = (ps - SCORE_PS[0]) / (SCORE_PS[1] - SCORE_PS[0]) * 100\n",
    "    return int(normalized_ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_offensive_performance_nba(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_offensive_performance_nfl(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_offensive_performance_mlb(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_offensive_performance_mls(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_offensive_performance_nhl(row):\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_defensive_performance_nba(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_defensive_performance_nfl(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_defensive_performance_mlb(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_defensive_performance_mls(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_defensive_performance_nhl(row):\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_measurables_nba(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_measurables_nfl(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_measurables_mlb(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_measurables_mls(row):\n",
    "    return np.nan\n",
    "\n",
    "def extract_measurables_nhl(row):\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBA\n",
    "league = \"NBA\"\n",
    "\n",
    "league_files[league] = {\n",
    "    \"Player\": extract_player_generic,\n",
    "    \"Player-additional\": extract_player_additional_generic,\n",
    "    'League': lambda row: league,\n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_nba,\n",
    "    \"Offensive Performance\": extract_offensive_performance_nba,\n",
    "    \"Defensive Performance\": extract_defensive_performance_nba,\n",
    "    \"Measurables\": extract_measurables_nba,\n",
    "    \"Migration Difficulty\": extract_migration_difficulty_generic\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFL\n",
    "league = \"NFL\"\n",
    "\n",
    "league_files[league] = {\n",
    "    \"Player\": extract_player_generic,\n",
    "    \"Player-additional\": extract_player_additional_generic,\n",
    "    'League': lambda row: league,\n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_nfl,\n",
    "    \"Offensive Performance\": extract_offensive_performance_nfl,\n",
    "    \"Defensive Performance\": extract_defensive_performance_nfl,\n",
    "    \"Measurables\": extract_measurables_nfl,\n",
    "    \"Migration Difficulty\": extract_migration_difficulty_generic\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLB\n",
    "league = \"MLB\"\n",
    "\n",
    "league_files[league] = {\n",
    "    \"Player\": extract_player_generic,\n",
    "    \"Player-additional\": extract_player_additional_generic,\n",
    "    'League': lambda row: league,    \n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_mlb,\n",
    "    \"Offensive Performance\": extract_offensive_performance_mlb,\n",
    "    \"Defensive Performance\": extract_defensive_performance_mlb,\n",
    "    \"Measurables\": extract_measurables_mlb,\n",
    "    \"Migration Difficulty\": extract_migration_difficulty_generic\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHL\n",
    "league = \"NHL\"\n",
    "\n",
    "league_files[league] = {\n",
    "    \"Player\": extract_player_generic,\n",
    "    \"Player-additional\": extract_player_additional_generic,\n",
    "    'League': lambda row: league,\n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_nhl,\n",
    "    \"Offensive Performance\": extract_offensive_performance_nhl,\n",
    "    \"Defensive Performance\": extract_defensive_performance_nhl,\n",
    "    \"Measurables\": extract_measurables_nhl,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS\n",
    "league = \"MLS\"\n",
    "\n",
    "league_files[league] = {\n",
    "    \"Player\": extract_player_generic,\n",
    "    \"Player-additional\": extract_player_additional_generic,\n",
    "    'League': lambda row: league,\n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_mls,\n",
    "    \"Offensive Performance\": extract_offensive_performance_mls,\n",
    "    \"Defensive Performance\": extract_defensive_performance_mls,\n",
    "    \"Measurables\": extract_measurables_mls,\n",
    "    \"Migration Difficulty\": extract_migration_difficulty_generic\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of column names to update\n",
    "# columns_to_update = None\n",
    "# columns_to_update = ['Player']\n",
    "columns_to_update = [('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')]\n",
    "# columns_to_update = ['Overall Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_merge(dataframes):\n",
    "    # Start with the first DataFrame as the base for merging\n",
    "    df_merged = dataframes[0]\n",
    "    \n",
    "    # Iterate through the remaining DataFrames\n",
    "    for df in dataframes[1:]:\n",
    "        # Determine the merge keys based on the 'Season' column's presence\n",
    "        if 'Season' in df.columns and 'Season' in df_merged.columns:\n",
    "            merge_keys = ['Season', 'Player-additional']\n",
    "        else:\n",
    "            merge_keys = ['Player-additional']\n",
    "        \n",
    "        # Perform the merge with the appropriate keys\n",
    "        df_merged = pd.merge(df_merged, df, on=merge_keys, how='outer', suffixes=('', '_dup'))\n",
    "        \n",
    "        # Remove duplicate columns resulting from merging\n",
    "        columns_to_drop = [col for col in df_merged.columns if '_dup' in col]\n",
    "        for col in columns_to_drop:\n",
    "            original_col = col.replace('_dup', '')\n",
    "            if df_merged[original_col].isna().all() and not df_merged[col].isna().all():\n",
    "                df_merged[original_col] = df_merged[col]\n",
    "            df_merged.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing league: NBA\n",
      "\tReading file: NBA_foreign.csv\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\tSaving combined data to ./formatted/NBA_formatted.csv...\n",
      "\n",
      "Processing league: NFL\n",
      "\tReading file: NFL_all.csv\n",
      "\tReading file: NFL_measure.csv\n",
      "\tReading file: NFL_births.csv\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\tSaving combined data to ./formatted/NFL_formatted.csv...\n",
      "\n",
      "Processing league: MLB\n",
      "\tReading file: MLB_B_foreign.csv\n",
      "\tReading file: MLB_P_foreign.csv\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\tSaving combined data to ./formatted/MLB_formatted.csv...\n",
      "\n",
      "Processing league: NHL\n",
      "\tReading file: NHL_foreign.csv\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\tSaving combined data to ./formatted/NHL_formatted.csv...\n",
      "\n",
      "Processing league: MLS\n",
      "\tReading file: MLS_foreign.csv\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\tSaving combined data to ./formatted/MLS_formatted.csv...\n",
      "\n",
      "Saving all leagues combined data to ./formatted/combined_formatted.csv...\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output directory exists\n",
    "output_directory = \"./formatted\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Initialize a list to store paths to the formatted CSV files\n",
    "formatted_csv_paths = []\n",
    "\n",
    "for league, filenames in files.items():\n",
    "    print(f\"Processing league: {league}\")\n",
    "    # Load the existing formatted CSV if it exists\n",
    "    output_path = os.path.join(output_directory, f\"{league}_formatted.csv\")\n",
    "    if os.path.exists(output_path):\n",
    "        df_final = pd.read_csv(output_path)\n",
    "    else:\n",
    "        # Initialize empty DataFrame with final columns if no file exists\n",
    "        df_final = pd.DataFrame(columns=final_columns)\n",
    "\n",
    "    dataframes = []\n",
    "    for filename in filenames:\n",
    "        print(f\"\\tReading file: {filename}\")\n",
    "        df = pd.read_csv(filename)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    df_merged = adaptive_merge(dataframes)\n",
    "\n",
    "    # Process the merged DataFrame\n",
    "    df_temp = pd.DataFrame()\n",
    "    for column_names, function in league_files[league].items():\n",
    "        # Skip columns that are not in the list of columns to update\n",
    "        if columns_to_update and column_names not in columns_to_update:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\t\\tSynthesizing '{column_names}'...\")\n",
    "        column_names = list(column_names) if isinstance(column_names, tuple) else column_names\n",
    "        df_temp[column_names] = df_merged.apply(function, axis=1, result_type='expand')\n",
    "\n",
    "    # Ensure the merged DataFrame has the columns from the processed data\n",
    "    df_final[df_temp.columns] = df_temp\n",
    "\n",
    "    # Select specific columns to save and handle missing columns gracefully\n",
    "    df_final = df_final.reindex(columns=final_columns)  # Reindex to ensure all desired columns are present\n",
    "    df_final = df_final.fillna(np.nan)  # Fill NA values\n",
    "    df_final = df_final.infer_objects()  # Infer data types to ensure consistent type handling\n",
    "\n",
    "    # Save to a new CSV file for the entire league\n",
    "    print(f\"\\tSaving combined data to {output_path}...\")\n",
    "    save_geo_dataframes()\n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    formatted_csv_paths.append(output_path)\n",
    "    print()\n",
    "\n",
    "# Combine all formatted CSV files into one big DataFrame and save it\n",
    "combined_df = pd.concat([pd.read_csv(f) for f in formatted_csv_paths], ignore_index=True)\n",
    "combined_output_path = os.path.join(output_directory, \"combined_formatted.csv\")\n",
    "print(f\"Saving all leagues combined data to {combined_output_path}...\")\n",
    "combined_df.to_csv(combined_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
