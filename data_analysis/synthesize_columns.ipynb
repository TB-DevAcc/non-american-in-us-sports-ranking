{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import country_converter as coco\n",
    "from typing import Optional, Tuple\n",
    "import logging\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
    "import warnings\n",
    "from babel.languages import get_territory_language_info\n",
    "\n",
    "# Suppress only FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "coco_logger = coco.logging.getLogger()\n",
    "coco_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Using geopy to get geographical coordinates\n",
    "geolocator = Nominatim(user_agent=\"MyPersonalGeocoder/1.0 (myemail@example.com)\", timeout=10)\n",
    "\n",
    "# Dictionary mapping file names to their respective processing functions\n",
    "files = {\n",
    "  \"NBA\": [\"NBA_foreign.csv\", \"NBA_height.csv\", \"NBA_weight.csv\"],\n",
    "  \"NFL\": [\"NFL_all.csv\", \"NFL_measure.csv\", \"NFL_births.csv\"],\n",
    "  \"MLB\": [\"MLB_foreign.csv\", \"MLB_measure.csv\"],\n",
    "  \"NHL\": [\"NHL_foreign.csv\", \"NHL_measure.csv\"],\n",
    "  \"MLS\": [\"MLS_foreign.csv\"],\n",
    "}\n",
    "league_files = {}\n",
    "\n",
    "# Global list of final column names\n",
    "final_columns = ['Player', 'Player-additional', 'League', 'Season', 'Home Continent', 'Home Country', 'Home City', 'Overall Value', 'Offensive Performance', 'Defensive Performance', 'Biometrics', 'Migration Difficulty']\n",
    "import pandas as pd\n",
    "\n",
    "# Define the data types for each column\n",
    "final_columns_dtype_mapping = {\n",
    "    'Player': 'object',\n",
    "    'Player-additional': 'object',\n",
    "    'League': 'object',\n",
    "    'Season': 'object', \n",
    "    'Home Continent': 'object',\n",
    "    'Home Country': 'object',\n",
    "    'Home City': 'object',\n",
    "    'Overall Value': 'float',\n",
    "    'Offensive Performance': 'float',\n",
    "    'Defensive Performance': 'float',\n",
    "    'Biometrics': 'float',\n",
    "    'Migration Difficulty': 'float'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaccessable_columns = {\n",
    "    \"Unknown\": set(),\n",
    "    }\n",
    "\n",
    "def get_row_value(row, column_name: str, type_=str):\n",
    "    try:\n",
    "        value = row[column_name]\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        if not isinstance(value, type_):\n",
    "            try:\n",
    "                return type_(value)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "    except KeyError:\n",
    "        inaccessable_columns[\"Unknown\"].add(column_name)\n",
    "        return np.nan\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError accessing row[{column_name}]: {str(e)}, with row {row}\")\n",
    "        return np.nan\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for the CSV files\n",
    "country_df_path = 'countries.csv'\n",
    "location_df_path = 'locations.csv'\n",
    "distance_df_path = 'distances.csv'\n",
    "\n",
    "# Load or initialize the country dataframe\n",
    "if os.path.exists(country_df_path):\n",
    "    country_df = pd.read_csv(country_df_path)\n",
    "else:\n",
    "    country_df = pd.DataFrame(columns=['code', 'country_name', 'continent', 'language', 'language_tier'])\n",
    "\n",
    "# Load or initialize the location dataframe\n",
    "if os.path.exists(location_df_path):\n",
    "    location_df = pd.read_csv(location_df_path)\n",
    "else:\n",
    "    location_df = pd.DataFrame(columns=['city', 'country', 'latitude', 'longitude'])\n",
    "\n",
    "# Load or initialize the distance dataframe\n",
    "if os.path.exists(distance_df_path):\n",
    "    distance_df = pd.read_csv(distance_df_path)\n",
    "else:\n",
    "    distance_df = pd.DataFrame(columns=['city_coords', 'sf_distance', 'ny_distance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_name(code: str):\n",
    "    global country_df\n",
    "    \n",
    "    match = country_df[country_df['code'] == code]\n",
    "    if not match.empty and not pd.isna(match['country_name'].iloc[0]):\n",
    "        return match['country_name'].iloc[0]\n",
    "    \n",
    "    # Call coco if no data is found or if the country name is NaN\n",
    "    country_name = coco.convert(names=code, to='name_short')\n",
    "    if match.empty:\n",
    "        new_row = pd.DataFrame({'code': [code], 'country_name': [country_name], 'continent': [pd.NA]})\n",
    "        country_df = pd.concat([country_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        country_df.loc[country_df['code'] == code, 'country_name'] = country_name\n",
    "    return country_name\n",
    "\n",
    "def get_continent(code: str):\n",
    "    global country_df\n",
    "    \n",
    "    match = country_df[country_df['code'] == code]\n",
    "    if not match.empty and 'continent' in match.columns and not pd.isna(match['continent'].iloc[0]):\n",
    "        return match['continent'].iloc[0]\n",
    "    \n",
    "    # Call coco if no data is found or continent is NaN\n",
    "    continent = coco.convert(names=code, to='Continent_7')\n",
    "    if match.empty:\n",
    "        new_row = pd.DataFrame({'code': [code], 'country_name': [pd.NA], 'continent': [continent]})\n",
    "        country_df = pd.concat([country_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        country_df.loc[country_df['code'] == code, 'continent'] = continent\n",
    "    return continent\n",
    "\n",
    "def lookup_location(city: str, country: str) -> Tuple[float, float]:\n",
    "    global location_df\n",
    "\n",
    "    match = location_df[(location_df['city'] == city) & (location_df['country'] == country)]\n",
    "    if not match.empty:\n",
    "        return match[['latitude', 'longitude']].iloc[0].tolist()\n",
    "    \n",
    "    try:\n",
    "        location = geolocator.geocode(f\"{city}, {country}\")\n",
    "        if location:\n",
    "            new_row = pd.DataFrame({'city': [city], 'country': [country], 'latitude': [location.latitude], 'longitude': [location.longitude]})\n",
    "            location_df = pd.concat([location_df, new_row], ignore_index=True)\n",
    "            return (location.latitude, location.longitude)\n",
    "    except (GeocoderTimedOut, GeocoderUnavailable) as e:\n",
    "        print(f\"Geocoding error due to timeout or unavailability: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_sf_distance(city_coords: Tuple[float, float], san_francisco_coords: Tuple[float, float] = (37.7749, -122.4194)):\n",
    "    global distance_df\n",
    "\n",
    "    # Convert city coordinates to a string format for easy comparison\n",
    "    city_coords_str = f\"{city_coords[0]},{city_coords[1]}\"\n",
    "\n",
    "    # Check if the distance already exists in the dataframe\n",
    "    match = distance_df[distance_df['city_coords'] == city_coords_str]\n",
    "    if not match.empty and not pd.isna(match['sf_distance'].iloc[0]):\n",
    "        return match['sf_distance'].iloc[0]\n",
    "    \n",
    "    # Calculate distance if no match is found\n",
    "    distance = geodesic(city_coords, san_francisco_coords).kilometers\n",
    "    if match.empty:\n",
    "        # Add new row if no entry exists for these city coordinates\n",
    "        new_row = pd.DataFrame({'city_coords': [city_coords_str], 'sf_distance': [distance], 'ny_distance': [pd.NA]})\n",
    "        distance_df = pd.concat([distance_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        # Update existing row with SF distance if it was NaN\n",
    "        distance_df.loc[distance_df['city_coords'] == city_coords_str, 'sf_distance'] = distance\n",
    "    return distance\n",
    "\n",
    "def get_ny_distance(city_coords: Tuple[float, float], new_york_coords: Tuple[float, float] = (40.7128, -74.0060)):\n",
    "    global distance_df\n",
    "\n",
    "    city_coords_str = f\"{city_coords[0]},{city_coords[1]}\"\n",
    "\n",
    "    match = distance_df[distance_df['city_coords'] == city_coords_str]\n",
    "    if not match.empty and not pd.isna(match['ny_distance'].iloc[0]):\n",
    "        return match['ny_distance'].iloc[0]\n",
    "    \n",
    "    # Calculate distance if no match is found\n",
    "    distance = geodesic(city_coords, new_york_coords).kilometers\n",
    "    if match.empty:\n",
    "        # Add new row if no entry exists for these city coordinates\n",
    "        new_row = pd.DataFrame({'city_coords': [city_coords_str], 'sf_distance': [pd.NA], 'ny_distance': [distance]})\n",
    "        distance_df = pd.concat([distance_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        # Update existing row with NY distance if it was NaN\n",
    "        distance_df.loc[distance_df['city_coords'] == city_coords_str, 'ny_distance'] = distance\n",
    "    return distance\n",
    "\n",
    "def get_distance(city, country):\n",
    "    city_coords = lookup_location(city, country)\n",
    "    if city_coords:\n",
    "        distance_to_sf = get_sf_distance(city_coords)\n",
    "        distance_to_ny = get_ny_distance(city_coords)\n",
    "        distance = min(distance_to_sf, distance_to_ny)\n",
    "    else:\n",
    "        distance = 0\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language_tiers = {\n",
    "#     \"English\": 0,\n",
    "#     \"Afrikaans\": 1, \"Danish\": 1, \"Dutch\": 1, \"French\": 1, \"Italian\": 1, \n",
    "#     \"Norwegian\": 1, \"Portuguese\": 1, \"Romanian\": 1, \"Spanish\": 1, \"Swedish\": 1,\n",
    "#     \"German\": 2,\n",
    "#     \"Indonesian\": 3, \"Malaysian\": 3, \"Swahili\": 3,\n",
    "#     \"Albanian\": 4, \"Amharic\": 4, \"Armenian\": 4, \"Azerbaijani\": 4, \"Bengali\": 4, \n",
    "#     \"Bosnian\": 4, \"Bulgarian\": 4, \"Burmese\": 4, \"Croatian\": 4, \"Czech\": 4,\n",
    "#     \"Estonian\": 4, \"Finnish\": 4, \"Georgian\": 4, \"Greek\": 4, \"Hebrew\": 4, \n",
    "#     \"Hindi\": 4, \"Hungarian\": 4, \"Icelandic\": 4, \"Khmer\": 4, \"Lao\": 4, \n",
    "#     \"Latvian\": 4, \"Lithuanian\": 4, \"Macedonian\": 4, \"Mongolian\": 4, \n",
    "#     \"Nepali\": 4, \"Pashto\": 4, \"Persian\": 4, \"Polish\": 4, \"Russian\": 4, \n",
    "#     \"Serbian\": 4, \"Sinhala\": 4, \"Slovak\": 4, \"Slovenian\": 4, \"Tagalog\": 4, \n",
    "#     \"Thai\": 4, \"Turkish\": 4, \"Ukrainian\": 4, \"Urdu\": 4, \"Uzbek\": 4, \n",
    "#     \"Vietnamese\": 4, \"Xhosa\": 4, \"Zulu\": 4,\n",
    "#     \"Arabic\": 5, \"Cantonese\": 5, \"Mandarin\": 5, \"Japanese\": 5, \"Korean\": 5\n",
    "# }\n",
    "\n",
    "language_tiers = {\n",
    "    \"en\": 0,  # English\n",
    "    \"af\": 1,  # Afrikaans\n",
    "    \"da\": 1,  # Danish\n",
    "    \"nl\": 1,  # Dutch\n",
    "    \"fr\": 1,  # French\n",
    "    \"it\": 1,  # Italian\n",
    "    \"no\": 1,  # Norwegian\n",
    "    \"pt\": 1,  # Portuguese\n",
    "    \"ro\": 1,  # Romanian\n",
    "    \"es\": 1,  # Spanish\n",
    "    \"sv\": 1,  # Swedish\n",
    "    \"de\": 2,  # German\n",
    "    \"id\": 3,  # Indonesian\n",
    "    \"ms\": 3,  # Malaysian\n",
    "    \"sw\": 3,  # Swahili\n",
    "    \"sq\": 4,  # Albanian\n",
    "    \"am\": 4,  # Amharic\n",
    "    \"hy\": 4,  # Armenian\n",
    "    \"az\": 4,  # Azerbaijani\n",
    "    \"bn\": 4,  # Bengali\n",
    "    \"bs\": 4,  # Bosnian\n",
    "    \"bg\": 4,  # Bulgarian\n",
    "    \"my\": 4,  # Burmese\n",
    "    \"hr\": 4,  # Croatian\n",
    "    \"cs\": 4,  # Czech\n",
    "    \"et\": 4,  # Estonian\n",
    "    \"fi\": 4,  # Finnish\n",
    "    \"ka\": 4,  # Georgian\n",
    "    \"el\": 4,  # Greek\n",
    "    \"he\": 4,  # Hebrew\n",
    "    \"hi\": 4,  # Hindi\n",
    "    \"hu\": 4,  # Hungarian\n",
    "    \"is\": 4,  # Icelandic\n",
    "    \"km\": 4,  # Khmer\n",
    "    \"lo\": 4,  # Lao\n",
    "    \"lv\": 4,  # Latvian\n",
    "    \"lt\": 4,  # Lithuanian\n",
    "    \"mk\": 4,  # Macedonian\n",
    "    \"mn\": 4,  # Mongolian\n",
    "    \"ne\": 4,  # Nepali\n",
    "    \"ps\": 4,  # Pashto\n",
    "    \"fa\": 4,  # Persian\n",
    "    \"pl\": 4,  # Polish\n",
    "    \"ru\": 4,  # Russian\n",
    "    \"sr\": 4,  # Serbian\n",
    "    \"si\": 4,  # Sinhala\n",
    "    \"sk\": 4,  # Slovak\n",
    "    \"sl\": 4,  # Slovenian\n",
    "    \"tl\": 4,  # Tagalog (Filipino)\n",
    "    \"th\": 4,  # Thai\n",
    "    \"tr\": 4,  # Turkish\n",
    "    \"uk\": 4,  # Ukrainian\n",
    "    \"ur\": 4,  # Urdu\n",
    "    \"uz\": 4,  # Uzbek\n",
    "    \"vi\": 4,  # Vietnamese\n",
    "    \"xh\": 4,  # Xhosa\n",
    "    \"zu\": 4,  # Zulu\n",
    "    \"ar\": 5,  # Arabic\n",
    "    \"yue\": 5, # Cantonese\n",
    "    \"zh\": 5,  # Mandarin\n",
    "    \"ja\": 5,  # Japanese\n",
    "    \"ko\": 5   # Korean\n",
    "}\n",
    "\n",
    "def get_lowest_tier_language(code: str):\n",
    "    global country_df\n",
    "    \n",
    "    # Convert the provided code to ISO alpha-2 for consistency in DataFrame\n",
    "    alpha_2_code = coco.convert(names=code, to='ISO2')\n",
    "    \n",
    "    # First, check if the data already exists in the DataFrame\n",
    "    match = country_df[country_df['code'] == code]\n",
    "    if not match.empty and not pd.isna(match['language'].iloc[0]) and not pd.isna(match['language_tier'].iloc[0]):\n",
    "        return match['language'].iloc[0], match['language_tier'].iloc[0]\n",
    "\n",
    "    # Fetch all languages spoken in the territory along with their details\n",
    "    languages_info = get_territory_language_info(alpha_2_code)\n",
    "    \n",
    "    # Filter to include only official languages with significant population percentage\n",
    "    official_languages = {\n",
    "        lang: info for lang, info in languages_info.items()\n",
    "        if info.get('official_status') and info.get('population_percent', 0) >= 30\n",
    "    }\n",
    "\n",
    "    if not official_languages:\n",
    "        return None, None\n",
    "\n",
    "    # Assuming language_tiers dictionary maps language codes to their respective tiers\n",
    "    default_tier = 5\n",
    "    lowest_tier_language = None\n",
    "    lowest_tier = default_tier\n",
    "\n",
    "    # Determine the language with the lowest tier\n",
    "    for language, _ in official_languages.items():\n",
    "        language_tier = language_tiers.get(language, default_tier)\n",
    "        if language_tier <= lowest_tier:\n",
    "            lowest_tier = language_tier\n",
    "            lowest_tier_language = language\n",
    "\n",
    "    # Update the DataFrame with new data if not found\n",
    "    if match.empty:\n",
    "        new_row = pd.DataFrame({\n",
    "            'code': [code], \n",
    "            'language': [lowest_tier_language], \n",
    "            'language_tier': [lowest_tier]\n",
    "        })\n",
    "        country_df = pd.concat([country_df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        country_df.loc[country_df['code'] == code, 'language'] = lowest_tier_language\n",
    "        country_df.loc[country_df['code'] == code, 'language_tier'] = lowest_tier\n",
    "\n",
    "    return lowest_tier_language, lowest_tier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_geo_dataframes():\n",
    "    country_df.to_csv(country_df_path, index=False)\n",
    "    location_df.to_csv(location_df_path, index=False)\n",
    "    distance_df.to_csv(distance_df_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "\n",
    "def extract_player_additional_generic(row):\n",
    "    player = get_row_value(row, \"Player\", type_=str)\n",
    "    player_add =  get_row_value(row, \"Player-additional\", type_=str)\n",
    "    season = get_row_value(row, \"Season\", type_=str)\n",
    "    return (player, player_add, season)\n",
    "\n",
    "\n",
    "# Define a dictionary for U.S. and Canadian state codes to their country ISO codes\n",
    "state_to_country = {\n",
    "    # U.S. state codes (abbreviated for brevity)\n",
    "    \"AL\": \"USA\", \"AK\": \"USA\", \"AZ\": \"USA\", \"AR\": \"USA\", \"CA\": \"USA\",\n",
    "    \"CO\": \"USA\", \"CT\": \"USA\", \"DE\": \"USA\", \"FL\": \"USA\", \"GA\": \"USA\",\n",
    "    \"HI\": \"USA\", \"ID\": \"USA\", \"IL\": \"USA\", \"IN\": \"USA\", \"IA\": \"USA\",\n",
    "    \"KS\": \"USA\", \"KY\": \"USA\", \"LA\": \"USA\", \"ME\": \"USA\", \"MD\": \"USA\",\n",
    "    \"MA\": \"USA\", \"MI\": \"USA\", \"MN\": \"USA\", \"MS\": \"USA\", \"MO\": \"USA\",\n",
    "    \"MT\": \"USA\", \"NE\": \"USA\", \"NV\": \"USA\", \"NH\": \"USA\", \"NJ\": \"USA\",\n",
    "    \"NM\": \"USA\", \"NY\": \"USA\", \"NC\": \"USA\", \"ND\": \"USA\", \"OH\": \"USA\",\n",
    "    \"OK\": \"USA\", \"OR\": \"USA\", \"PA\": \"USA\", \"RI\": \"USA\", \"SC\": \"USA\",\n",
    "    \"SD\": \"USA\", \"TN\": \"USA\", \"TX\": \"USA\", \"UT\": \"USA\", \"VT\": \"USA\",\n",
    "    \"VA\": \"USA\", \"WA\": \"USA\", \"WV\": \"USA\", \"WI\": \"USA\", \"WY\": \"USA\",\n",
    "    # Canadian province codes\n",
    "    \"AB\": \"CAN\", \"BC\": \"CAN\", \"MB\": \"CAN\", \"NB\": \"CAN\", \"NL\": \"CAN\",\n",
    "    \"NS\": \"CAN\", \"ON\": \"CAN\", \"PE\": \"CAN\", \"QC\": \"CAN\", \"SK\": \"CAN\",\n",
    "    \"NT\": \"CAN\", \"NU\": \"CAN\", \"YT\": \"CAN\",\n",
    "    # Other codes\n",
    "    \"SUN\": \"RUS\", \"CRK\": \"CZE\", \"ENG\": \"GBR\", \"CSK\": \"SVK\", \"DDR\": \"DEU\", \"YUG\": \"SRB\"\n",
    "}\n",
    "\n",
    "# Earth circumference assumptions\n",
    "MAX_DISTANCE = 20037.5  # Half the Earth's circumference in km\n",
    "\n",
    "def extract_location_generic(row) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extracts values for columns 'Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"\n",
    "    \"\"\"\n",
    "    code = get_row_value(row, 'Birth Location', type_=str)\n",
    "    if pd.isna(code) or \"\":\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "    # Split from the back and only split once\n",
    "    city, code = code.rsplit(' ', 1)\n",
    "\n",
    "    # First check against state codes, then use country_converter if needed\n",
    "    code = state_to_country.get(code, code)\n",
    "\n",
    "    # Find country names\n",
    "    country = get_country_name(code)\n",
    "    if country == \"not_found\":\n",
    "        print(f\"Country code {code} not found in row {row}\")\n",
    "        country = np.nan\n",
    "\n",
    "    # Convert code to continent\n",
    "    continent = get_continent(code)\n",
    "    if continent == \"not_found\":\n",
    "        print(f\"Continent code {code} not found in row {row}\")\n",
    "        continent = np.nan\n",
    "\n",
    "    language, language_tier = get_lowest_tier_language(code)\n",
    "    if not language or not language_tier:\n",
    "        language = np.nan\n",
    "        language_tier = 0\n",
    "\n",
    "    language_tier = language_tier / 5 # normalize to [0, 1]\n",
    "\n",
    "    distance = get_distance(city, country)\n",
    "    distance_norm = distance / MAX_DISTANCE # normalize to [0, 1]\n",
    "\n",
    "    migration = f\"{language}:{language_tier} {distance}:{distance_norm}\"\n",
    "\n",
    "    return (continent, country, city, migration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Value\n",
    "\n",
    "# Constants for maximum values used in normalization\n",
    "SCORE_WS = (-3, 26) # Win Shares (Woody Sauldsberry -2.8 1960-61) - (Kareem Abdul-Jabbar 25.4 1971-72)\n",
    "SCORE_AV = (-6, 26) # Approximate Value (Dan Pastorini -6 1981) - (LaDainian Tomlinson 26 2006)\n",
    "SCORE_WAR = (-6, 21) # Wins Above Replacement (Jersey Bakley -5.3 1884) - (Pud Galvin 20.5 1884)\n",
    "PLUS_MINUS_MLS = (-38, 48) # Plus-Minus (Mathieu Deplagne -38 2019) - (Carlos Vela 48 2019)\n",
    "SCORE_PS = (-2.4, 23) # Point Shares (Ken Baumgartner -2.4 1997-98) - (Bobby Orr 22.8 1970-71)\n",
    "\n",
    "def extract_overall_value_nba(row):\n",
    "    win_shares = get_row_value(row, 'WS', float)\n",
    "    all_star_appearances = get_row_value(row, 'AS', float)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    win_shares = 0 if np.isnan(win_shares) else win_shares\n",
    "    all_star_appearances = 0 if np.isnan(all_star_appearances) else all_star_appearances\n",
    "\n",
    "    # Normalize with new range\n",
    "    win_shares_norm = (win_shares - SCORE_WS[0]) / (SCORE_WS[1] - SCORE_WS[0]) * 100\n",
    "    score_all_star = all_star_appearances * 100\n",
    "\n",
    "    return f\"{win_shares}:{win_shares_norm} {all_star_appearances}:{score_all_star}\"\n",
    "\n",
    "def extract_overall_value_nfl(row):\n",
    "    av = get_row_value(row, 'AV', float)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    av = 0 if np.isnan(av) else av\n",
    "\n",
    "    # Normalize with new range\n",
    "    av_score = (av - SCORE_AV[0]) / (SCORE_AV[1] - SCORE_AV[0]) * 100\n",
    "    return f\"{av}:{av_score} \"\n",
    "\n",
    "def extract_overall_value_mlb(row):\n",
    "    war = get_row_value(row, 'WAR', float)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    war = 0 if np.isnan(war) else war\n",
    "\n",
    "    # Normalize with new range\n",
    "    war_score = (war - SCORE_WAR[0]) / (SCORE_WAR[1] - SCORE_WAR[0]) * 100\n",
    "    return f\"{war}:{war_score} \"\n",
    "\n",
    "def extract_overall_value_mls(row):\n",
    "    plus_minus = get_row_value(row, '+/-', int)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    plus_minus = 0 if np.isnan(plus_minus) else plus_minus\n",
    "\n",
    "    # Normalize with new range\n",
    "    plus_minus_score = (plus_minus - PLUS_MINUS_MLS[0]) / (PLUS_MINUS_MLS[1] - PLUS_MINUS_MLS[0]) * 100\n",
    "    return f\"{plus_minus}:{plus_minus_score} \"\n",
    "\n",
    "def extract_overall_value_nhl(row):\n",
    "    ps = get_row_value(row, 'PS', float)\n",
    "\n",
    "    # Convert NaN to 0\n",
    "    ps = 0 if np.isnan(ps) else ps\n",
    "\n",
    "    # Normalize with new range\n",
    "    ps_score = (ps - SCORE_PS[0]) / (SCORE_PS[1] - SCORE_PS[0]) * 100\n",
    "    return f\"{ps}:{ps_score} \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for maximum values used in normalization\n",
    "MIN_NBA_PTS, MAX_NBA_PTS = 0, 4029 # Points (Wilt Chamberlain 4029 1961-62)\n",
    "MIN_NBA_PPG, MAX_NBA_PPG = 0, 51 # Points per Game (Wilt Chamberlain 50.4 1961-62)\n",
    "MIN_NBA_AST, MAX_NBA_AST = 0, 1164 # Assists (John Stockton 1164 1990-91)\n",
    "MIN_NBA_APG, MAX_NBA_APG = 0, 15 # Assists per Game (John Stockton 14.5 1989-90)\n",
    "\n",
    "MIN_NFL_PTS, MAX_NFL_PTS = 0, 186 # Points (LaDainian Tomlinson 186 2006)\n",
    "MIN_NFL_PPG, MAX_NFL_PPG = 0, 14.7 # Points per Game (Paul Hornung 176 12 Games 1960)\n",
    "MIN_NFL_YSCM, MAX_NFL_YSCM = -23, 2509 # Yards from Scrimmage (Chris Johnson 2509 2009)\n",
    "MIN_NFL_YSCMPG, MAX_NFL_YSCMPG = 0, 157 # Yards from Scrimmage per Game (Chris Johnson 2509 16 Games 2009)\n",
    "MIN_NFL_PYDS, MAX_NFL_PYDS = -4, 5477 # Passing Yards (Peyton Manning 5477 2013)\n",
    "MIN_NFL_PYDSPG, MAX_NFL_PYDSPG = -4, 343 # Passing Yards per Game (Peyton Manning 5477 16 Games 2013)\n",
    "\n",
    "MIN_MLB_HR, MAX_MLB_HR = 0, 73 # Home Runs (Barry Bonds 73 2001)\n",
    "MIN_MLB_HRPG, MAX_MLB_HRPG = 0, 0.5 # Home Runs per Game (Barry Bonds 73 153 Games 2001)\n",
    "MIN_MLB_RBI, MAX_MLB_RBI = 0, 160 # Runs Batted In (Hack Wilson 191 1930)\n",
    "MIN_MLB_RBIPG, MAX_MLB_RBIPG = 0, 1.25 # Runs Batted In per Game (Hack Wilson 191 155 Games 1930)\n",
    "\n",
    "MIN_MLS_GOALS, MAX_MLS_GOALS = 0, 34 # Goals (Carlos Vela 34 2019)\n",
    "MIN_MLS_GOALSPG, MAX_MLS_GOALSPG = 0, 1.1 # Goals per Game (Carlos Vela 34 31 Games 2019)\n",
    "MIN_MLS_AST, MAX_MLS_AST = 0, 21 # Assists (Carlos Valderrama 21 2000)\n",
    "MIN_MLS_ASTPG, MAX_MLS_ASTPG = 0, 1.0 # Assists per Game (Lionel Messi 10 10 Games 2024)\n",
    "\n",
    "MIN_NHL_POINTS, MAX_NHL_POINTS = 0, 215 # Points (Wayne Gretzky 215 1985-86)\n",
    "MIN_NHL_POINTSPG, MAX_NHL_POINTSPG = 0, 2.7 # Points per Game (Wayne Gretzky 215 80 Games 1985-86)\n",
    "MIN_NHL_GOALS, MAX_NHL_GOALS = 0, 92 # Goals (Wayne Gretzky 92 1981-82)\n",
    "MIN_NHL_GOALSPG, MAX_NHL_GOALSPG = 0, 1.15 # Goals per Game (Wayne Gretzky 92 80 Games 1981-82)\n",
    "\n",
    "def extract_offensive_performance_nba(row):\n",
    "    g = get_row_value(row, 'G', float)\n",
    "    pts = get_row_value(row, 'PTS', float)\n",
    "    ast = get_row_value(row, 'AST', float)\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(pts):\n",
    "        pts = 0\n",
    "    if pd.isna(ast):\n",
    "        ast = 0\n",
    "    \n",
    "    ppg = pts / g\n",
    "    apg = ast / g\n",
    "    p_score = 100 * (pts - MIN_NBA_PTS) / (MAX_NBA_PTS - MIN_NBA_PTS)\n",
    "    a_score = 100 * (ast - MIN_NBA_AST) / (MAX_NBA_AST - MIN_NBA_AST)\n",
    "    ppg_score = 100 * (ppg - MIN_NBA_PPG) / (MAX_NBA_PPG - MIN_NBA_PPG)\n",
    "    apg_score = 100 * (apg - MIN_NBA_APG) / (MAX_NBA_APG - MIN_NBA_APG)\n",
    "    return f\"{pts}:{p_score} {ast}:{a_score} {ppg}:{ppg_score} {apg}:{apg_score}\"\n",
    "\n",
    "def extract_offensive_performance_nfl(row):\n",
    "    g = get_row_value(row, 'G', float)\n",
    "    pts = get_row_value(row, 'Pts', int) # Total Points from all means\n",
    "    yscm = get_row_value(row, 'YScm', int)  # Yards from Scrimmage\n",
    "    pyds = get_row_value(row, 'Yds', int)  # Passing Yards\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(pts):\n",
    "        pts = 0\n",
    "    if pd.isna(yscm):\n",
    "        yscm = 0\n",
    "    if pd.isna(pyds):\n",
    "        pyds = 0\n",
    "\n",
    "    ppg = pts / g\n",
    "    yscmpg = yscm / g\n",
    "    pydspg = pyds / g\n",
    "    pts_score = 100 * (pts - MIN_NFL_PTS) / (MAX_NFL_PTS - MIN_NFL_PTS)\n",
    "    yscm_score = 100 * (yscm - MIN_NFL_YSCM) / (MAX_NFL_YSCM - MIN_NFL_YSCM)\n",
    "    pyds_score = 100 * (pyds - MIN_NFL_PYDS) / (MAX_NFL_PYDS - MIN_NFL_PYDS)\n",
    "    ppg_score = 100 * (ppg - MIN_NFL_PPG) / (MAX_NFL_PPG - MIN_NFL_PPG)\n",
    "    yscmpg_score = 100 * (yscmpg - MIN_NFL_YSCMPG) / (MAX_NFL_YSCMPG - MIN_NFL_YSCMPG)\n",
    "    pydspg_score = 100 * (pydspg - MIN_NFL_PYDSPG) / (MAX_NFL_PYDSPG - MIN_NFL_PYDSPG)\n",
    "    return f\"{pts}:{pts_score} {yscm}:{yscm_score} {pyds}:{pyds_score} {ppg}:{ppg_score} {yscmpg}:{yscmpg_score} {pydspg}:{pydspg_score}\"\n",
    "\n",
    "def extract_offensive_performance_mlb(row):\n",
    "    g = get_row_value(row, 'G', float)\n",
    "    hr = get_row_value(row, 'HR', int)\n",
    "    rbi = get_row_value(row, 'RBI', int)\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(hr):\n",
    "        hr = 0\n",
    "    if pd.isna(rbi):\n",
    "        rbi = 0\n",
    "\n",
    "    hrpg = hr / g\n",
    "    rbipg = rbi / g\n",
    "    hr_score = 100 * (hr - MIN_MLB_HR) / (MAX_MLB_HR - MIN_MLB_HR)\n",
    "    rbi_score = 100 * (rbi - MIN_MLB_RBI) / (MAX_MLB_RBI - MIN_MLB_RBI)\n",
    "    hrpg_score = 100 * (hrpg - MIN_MLB_HRPG) / (MAX_MLB_HRPG - MIN_MLB_HRPG)\n",
    "    rbipg_score = 100 * (rbipg - MIN_MLB_RBIPG) / (MAX_MLB_RBIPG - MIN_MLB_RBIPG)\n",
    "    return f\"{hr}:{hr_score} {rbi}:{rbi_score} {hrpg}:{hrpg_score} {rbipg}:{rbipg_score}\"\n",
    "\n",
    "\n",
    "def extract_offensive_performance_mls(row):\n",
    "    g = get_row_value(row, 'MP', float)\n",
    "    goals = get_row_value(row, 'Gls', int)\n",
    "    assists = get_row_value(row, 'Ast', int)\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(goals):\n",
    "        goals = 0\n",
    "    if pd.isna(assists):\n",
    "        assists = 0\n",
    "\n",
    "    goalspg = goals / g\n",
    "    assistspg = assists / g\n",
    "    goals_score = 100 * (goals - MIN_MLS_GOALS) / (MAX_MLS_GOALS - MIN_MLS_GOALS)\n",
    "    assists_score = 100 * (assists - MIN_MLS_AST) / (MAX_MLS_AST - MIN_MLS_AST)\n",
    "    goalspg_score = 100 * (goalspg - MIN_MLS_GOALSPG) / (MAX_MLS_GOALSPG - MIN_MLS_GOALSPG)\n",
    "    assistspg_score = 100 * (assistspg - MIN_MLS_ASTPG) / (MAX_MLS_ASTPG - MIN_MLS_ASTPG)\n",
    "    return f\"{goals}:{goals_score} {assists}:{assists_score} {goalspg}:{goalspg_score} {assistspg}:{assistspg_score}\"\n",
    "\n",
    "\n",
    "def extract_offensive_performance_nhl(row):\n",
    "    g = get_row_value(row, 'GP', float)\n",
    "    points = get_row_value(row, 'PTS', int)\n",
    "    goals = get_row_value(row, 'G', int)\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(points):\n",
    "        points = 0\n",
    "    if pd.isna(goals):\n",
    "        goals = 0\n",
    "\n",
    "    pointspg = points / g\n",
    "    goalspg = goals / g\n",
    "    points_score = 100 * (points - MIN_NHL_POINTS) / (MAX_NHL_POINTS - MIN_NHL_POINTS)\n",
    "    goals_score = 100 * (goals - MIN_NHL_GOALS) / (MAX_NHL_GOALS - MIN_NHL_GOALS)\n",
    "    pointspg_score = 100 * (pointspg - MIN_NHL_POINTSPG) / (MAX_NHL_POINTSPG - MIN_NHL_POINTSPG)\n",
    "    goalspg_score = 100 * (goalspg - MIN_NHL_GOALSPG) / (MAX_NHL_GOALSPG - MIN_NHL_GOALSPG)\n",
    "    return f\"{points}:{points_score} {goals}:{goals_score} {pointspg}{pointspg_score} {goalspg}:{goalspg_score}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NBA_BLK, MAX_NBA_BLK = 0, 456  # Blocks (Mark Eaton 456 in 1984-85)\n",
    "MIN_NBA_BPG, MAX_NBA_BPG = 0, 5.6  # Blocks Per Game (Mark Eaton 5.6 in 1984-85 82 Games)\n",
    "MIN_NBA_STL, MAX_NBA_STL = 0, 301  # Steals (Alvin Robertson 301 in 1985-86)\n",
    "MIN_NBA_SPG, MAX_NBA_SPG = 0, 3.7  # Steals Per Game (Alvin Robertson 3.7 in 1985-86 82 Games)\n",
    "MIN_NBA_DRB, MAX_NBA_DRB = 0, 1111  # Defensive Rebounds (Kareem Abdul-Jabbar 1111 in 1975-76)\n",
    "MIN_NBA_DRPG, MAX_NBA_DRPG = 0, 13.7  # Defensive Rebounds Per Game (Elvin Hayes 13.7 in 1973-74 81 Games)\n",
    "\n",
    "MIN_NFL_TKL, MAX_NFL_TKL = 0, 200  # Tackles combined (Hardy Nickerson 214 1993)\n",
    "MIN_NFL_TKLPG, MAX_NFL_TKLPG = 0, 13.4  # Tackles per Game (Hardy Nickerson 13.4 1993 16 Games)\n",
    "MIN_NFL_SK, MAX_NFL_SK = 0, 23  # Sacks (Al \"Bubba\" Baker 23 in 1978)\n",
    "MIN_NFL_SKPG, MAX_NFL_SKPG = 0, 1.75  # Sacks per Game (Reggie White 21 in 12 Games 1987)\n",
    "MIN_NFL_INT, MAX_NFL_INT = 0, 14  # Interceptions (Night Train Lane 14 in 1952)\n",
    "MIN_NFL_INTPG, MAX_NFL_INTPG = 0, 1.2  # Interceptions per Game (Night Train Lane 14 in 12 Games 1952)\n",
    "MIN_NFL_FF, MAX_NFL_FF = 0, 10  # Forced Fumbles (Robert Mathis 10 2013)\n",
    "MIN_NFL_FFPG, MAX_NFL_FFPG = 0, 0.63  # Forced Fumbles per Game (Robert Mathis 10 in 16 Games 2013)\n",
    "MIN_NFL_PD, MAX_NFL_PD = 0, 31  # Passes Defended (Darrelle Revis 31 2009)\n",
    "MIN_NFL_PDPG, MAX_NFL_PDPG = 0, 1.94  # Passes Defended per Game (Darrelle Revis 31 in 16 Games 2009)\n",
    "\n",
    "MIN_MLB_ERA, MAX_MLB_ERA = 189, 0  # Earned Run Average \n",
    "MIN_MLB_SO, MAX_MLB_SO = 0, 513  # Strikeouts (Matt Kilroy 513 1886)\n",
    "MIN_MLB_SOPG, MAX_MLB_SOPG = 0, 7.6  # Strikeouts per Game (Matt Kilroy 513 in 68 Games 1886)\n",
    "MIN_MLB_SHO, MAX_MLB_SHO = 0, 16  # Shutouts (Grover Alexander 16 1916)\n",
    "\n",
    "MIN_MLS_TKLWN, MAX_MLS_TKLWN = 0, 99  # Tackles won (Rasmus Schüller 99 2018)\n",
    "MIN_MLS_TKLWNPG, MAX_MLS_TKLWNPG = 0, 3.6  # Tackles won per game (Jimmy Medranda 3.6 2016)\n",
    "MIN_MLS_INT, MAX_MLS_INT = 0, 120  # Interceptions (Laurent Ciman 120 2016)\n",
    "MIN_MLS_INTPG, MAX_MLS_INTPG = 0, 4.9  # Interceptions per game (Víctor Cabrera 4.9 2016 over 23 Games)\n",
    "MIN_MLS_SVS, MAX_MLS_SVS = 0, 177  # Saves for goalkeepers (Scott Garlick 177 2000)\n",
    "MIN_MLS_SVSPG, MAX_MLS_SVSPG = 0, 5  # Saves per game for goalkeepers (Jay Nolly 7.0 2005 over 5 Games)\n",
    "\n",
    "MIN_NHL_HITS, MAX_NHL_HITS = 0, 386  # Hits (Jeremy Lauzon 386 2023)\n",
    "MIN_NHL_HITSPG, MAX_NHL_HITSPG = 0, 4.89  # Hits per Game (Jeremy Lauzon 386 2023 in 79 Games)\n",
    "MIN_NHL_BLOCK, MAX_NHL_BLOCK = 0, 283  # Blocked Shots (Kris Russell 283 2014)\n",
    "MIN_NHL_BLOCKPG, MAX_NHL_BLOCKPG = 0, 3.6  # Blocked Shots (Kris Russell 283 2014 in 79 Games)\n",
    "\n",
    "def extract_defensive_performance_nba(row):\n",
    "    g = get_row_value(row, 'G', float)\n",
    "    blk = get_row_value(row, 'BLK', float)\n",
    "    stl = get_row_value(row, 'STL', float)\n",
    "    drb = get_row_value(row, 'DRB', float)\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(blk):\n",
    "        blk = 0\n",
    "    if pd.isna(stl):\n",
    "        stl = 0\n",
    "    if pd.isna(drb):\n",
    "        drb = 0\n",
    "    \n",
    "    bpg = blk / g\n",
    "    spg = stl / g\n",
    "    drpg = drb / g\n",
    "    blk_score = 100 * (blk - MIN_NBA_BLK) / (MAX_NBA_BLK - MIN_NBA_BLK)\n",
    "    stl_score = 100 * (stl - MIN_NBA_STL) / (MAX_NBA_STL - MIN_NBA_STL)\n",
    "    drb_score = 100 * (drb - MIN_NBA_DRB) / (MAX_NBA_DRB - MIN_NBA_DRB)\n",
    "    bpg_score = 100 * (bpg - MIN_NBA_BPG) / (MAX_NBA_BPG - MIN_NBA_BPG)\n",
    "    spg_score = 100 * (spg - MIN_NBA_SPG) / (MAX_NBA_SPG - MIN_NBA_SPG)\n",
    "    drpg_score = 100 * (drpg - MIN_NBA_DRPG) / (MAX_NBA_DRPG - MIN_NBA_DRPG)\n",
    "    return f\"{blk}:{blk_score} {stl}:{stl_score} {drb}:{drb_score} {bpg}:{bpg_score} {spg}:{spg_score} {drpg}:{drpg_score}\"\n",
    "\n",
    "def extract_defensive_performance_nfl(row):\n",
    "    g = get_row_value(row, 'G', float)\n",
    "    tkl = get_row_value(row, 'Comb', int) # Combined tackles\n",
    "    sk = get_row_value(row, 'Sk', float)\n",
    "    intc = get_row_value(row, 'Int', int)\n",
    "    ff = get_row_value(row, 'FF', int)\n",
    "    passd = get_row_value(row, 'PD', int)\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(tkl):\n",
    "        tkl = 0\n",
    "    if pd.isna(sk):\n",
    "        sk = 0\n",
    "    if pd.isna(intc):\n",
    "        intc = 0\n",
    "    if pd.isna(ff):\n",
    "        ff = 0\n",
    "    if pd.isna(passd):\n",
    "        passd = 0\n",
    "    \n",
    "    tklpg = tkl / g\n",
    "    skpg = sk / g\n",
    "    intpg = intc / g\n",
    "    ffpg = ff / g\n",
    "    passdpg = passd / g\n",
    "    tkl_score = 100 * (tkl - MIN_NFL_TKL) / (MAX_NFL_TKL - MIN_NFL_TKL)\n",
    "    sk_score = 100 * (sk - MIN_NFL_SK) / (MAX_NFL_SK - MIN_NFL_SK)\n",
    "    int_score = 100 * (intc - MIN_NFL_INT) / (MAX_NFL_INT - MIN_NFL_INT)\n",
    "    ff_score = 100 * (ff - MIN_NFL_FF) / (MAX_NFL_FF - MIN_NFL_FF)\n",
    "    passd_score = 100 * (passd - MIN_NFL_PD) / (MAX_NFL_PD - MIN_NFL_PD)\n",
    "    tklpg_score = 100 * (tklpg - MIN_NFL_TKLPG) / (MAX_NFL_TKLPG - MIN_NFL_TKLPG)\n",
    "    skpg_score = 100 * (skpg - MIN_NFL_SKPG) / (MAX_NFL_SKPG - MIN_NFL_SKPG)\n",
    "    intpg_score = 100 * (intpg - MIN_NFL_INTPG) / (MAX_NFL_INTPG - MIN_NFL_INTPG)\n",
    "    ffpg_score = 100 * (ffpg - MIN_NFL_FFPG) / (MAX_NFL_FFPG - MIN_NFL_FFPG)\n",
    "    passdpg_score = 100 * (passdpg - MIN_NFL_PDPG) / (MAX_NFL_PDPG - MIN_NFL_PDPG)\n",
    "    return f\"{tkl}:{tkl_score} {sk}:{sk_score} {intc}:{int_score} {ff}:{ff_score} {passd}:{passd_score} {tklpg}:{tklpg_score} {skpg}:{skpg_score} {intpg}:{intpg_score} {ffpg}:{ffpg_score} {passdpg}:{passdpg_score}\"\n",
    "\n",
    "def extract_defensive_performance_mlb(row):\n",
    "    g = get_row_value(row, 'G', float)\n",
    "    era = get_row_value(row, 'ERA', float)\n",
    "    so = get_row_value(row, 'SO', int)\n",
    "    sho = get_row_value(row, 'SHO', int)\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(era):\n",
    "        era = 0\n",
    "    if pd.isna(so):\n",
    "        so = 0\n",
    "    if pd.isna(sho):\n",
    "        sho = 0\n",
    "\n",
    "    sopg = so / g\n",
    "    era_score = 100 * (MIN_MLB_ERA - era) / (MIN_MLB_ERA - MAX_MLB_ERA)  # Reverse since lower ERA is better\n",
    "    so_score = 100 * (so - MIN_MLB_SO) / (MAX_MLB_SO - MIN_MLB_SO)\n",
    "    sopg_score = 100 * (sopg - MIN_MLB_SOPG) / (MAX_MLB_SOPG - MIN_MLB_SOPG)\n",
    "    sho_score = 100 * (sho - MIN_MLB_SHO) / (MAX_MLB_SHO - MIN_MLB_SHO)\n",
    "    return f\"{era}:{era_score} {so}:{so_score} {sopg}:{sopg_score} {sho}:{sho_score}\"\n",
    "\n",
    "def extract_defensive_performance_mls(row):\n",
    "    g = get_row_value(row, 'MP', float)\n",
    "    tklwn = get_row_value(row, 'TklW', int)\n",
    "    intl = get_row_value(row, 'Int', int)\n",
    "    # svs = get_row_value(row, 'Svs', int)  # Assuming 'Svs' is used for goalkeeper saves\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(tklwn):\n",
    "        tklwn = 0\n",
    "    if pd.isna(intl):\n",
    "        intl = 0\n",
    "    # if pd.isna(svs):\n",
    "        # svs = 0\n",
    "\n",
    "    tklwnpg = tklwn / g\n",
    "    intpg = intl / g\n",
    "    # svspg = svs / g\n",
    "    tklwn_score = 100 * (tklwn - MIN_MLS_TKLWN) / (MAX_MLS_TKLWN - MIN_MLS_TKLWN)\n",
    "    intl_score = 100 * (intl - MIN_MLS_INT) / (MAX_MLS_INT - MIN_MLS_INT)\n",
    "    # svs_score = 100 * (svs - MIN_MLS_SVS) / (MAX_MLS_SVS - MIN_MLS_SVS)\n",
    "    tklwnpg_score = 100 * (tklwnpg - MIN_MLS_TKLWNPG) / (MAX_MLS_TKLWNPG - MIN_MLS_TKLWNPG)\n",
    "    intpg_score = 100 * (intpg - MIN_MLS_INTPG) / (MAX_MLS_INTPG - MIN_MLS_INTPG)\n",
    "    # svspg_score = 100 * (svspg - MIN_MLS_SVSPG) / (MAX_MLS_SVSPG - MIN_MLS_SVSPG)\n",
    "    return f\"{tklwn}:{tklwn_score} {intl}:{intl_score} {tklwnpg}:{tklwnpg_score} {intpg}:{intpg_score}\"\n",
    "\n",
    "def extract_defensive_performance_nhl(row):\n",
    "    g = get_row_value(row, 'GP', float)\n",
    "    hits = get_row_value(row, 'HIT', int)\n",
    "    blocks = get_row_value(row, 'BL', int)\n",
    "    if pd.isna(g) or g == 0:\n",
    "        return np.nan\n",
    "    if pd.isna(hits):\n",
    "        hits = 0\n",
    "    if pd.isna(blocks):\n",
    "        blocks = 0\n",
    "\n",
    "    hitspg = hits / g\n",
    "    blockspg = blocks / g\n",
    "    hits_score = 100 * (hits - MIN_NHL_HITS) / (MAX_NHL_HITS - MIN_NHL_HITS)\n",
    "    blocks_score = 100 * (blocks - MIN_NHL_BLOCK) / (MAX_NHL_BLOCK - MIN_NHL_BLOCK)\n",
    "    hitspg_score = 100 * (hitspg - MIN_NHL_HITSPG) / (MAX_NHL_HITSPG - MIN_NHL_HITSPG)\n",
    "    blockspg_score = 100 * (blockspg - MIN_NHL_BLOCKPG) / (MAX_NHL_BLOCKPG - MIN_NHL_BLOCKPG)\n",
    "    return f\"{hits}:{hits_score} {blocks}:{blocks_score} {hitspg}:{hitspg_score} {blockspg}:{blockspg_score}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_height_to_metric_height(height: str) -> float:\n",
    "  conv = [30.48, 2.54]\n",
    "  ht = [float(x) for x in height.split(\"-\")]\n",
    "  return round(np.dot(ht, conv), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for normalization\n",
    "MIN_NBA_HEIGHT, MAX_NBA_HEIGHT = imp_height_to_metric_height(\"5-3\"), imp_height_to_metric_height(\"7-7\") # Height in cm (Muggsy Bogues, Manute Bol)\n",
    "MIN_NBA_WEIGHT, MAX_NBA_WEIGHT = 133, 360   # Weight in lbs (Spud Webb, Sim Bhullar)\n",
    "\n",
    "MIN_NFL_HEIGHT, MAX_NFL_HEIGHT = imp_height_to_metric_height(\"5-1\"), imp_height_to_metric_height(\"7-0\") # Height in cm (Jack Shapiro, Richard Sligh)\n",
    "MIN_NFL_WEIGHT, MAX_NFL_WEIGHT = 119, 380   # Weight in lbs (Jack Shapiro - Daniel Faalele)\n",
    "\n",
    "MIN_MLB_HEIGHT, MAX_MLB_HEIGHT = imp_height_to_metric_height(\"4-5\"), imp_height_to_metric_height(\"6-11\") # Height in cm (Charlie Hughes - Sean Hjelle)\n",
    "MIN_MLB_WEIGHT, MAX_MLB_WEIGHT = 120, 320   # Weight in lbs (Candy Cummings - Walter Young)\n",
    "\n",
    "MIN_MLS_HEIGHT, MAX_MLS_HEIGHT = imp_height_to_metric_height(\"5-1\"), imp_height_to_metric_height(\"6-7\") # Height in cm (Aaron Herrera - Adam Bedell)\n",
    "MIN_MLS_WEIGHT, MAX_MLS_WEIGHT = 115, 236   # Weight in lbs (Luis Arriaga - Chris Seitz)\n",
    "\n",
    "MIN_NHL_HEIGHT, MAX_NHL_HEIGHT = imp_height_to_metric_height(\"5-4\"), imp_height_to_metric_height(\"6-9\") # Height in cm (Nathan Gerbe - Zdeno Chara)\n",
    "MIN_NHL_WEIGHT, MAX_NHL_WEIGHT = 133, 265   # Weight in lbs (Ken Doraty - Derek Boogaard)\n",
    "\n",
    "def extract_measurables_nhl(row):\n",
    "    height = get_row_value(row, 'Ht.', str)\n",
    "    weight = get_row_value(row, 'Wt.', float)  \n",
    "    \n",
    "    if pd.isna(height) or pd.isna(weight):\n",
    "        return np.nan\n",
    "    \n",
    "    height = imp_height_to_metric_height(height)\n",
    "    \n",
    "    # Normalize height and weight\n",
    "    height_score = 100 * (height - MIN_NHL_HEIGHT) / (MAX_NHL_HEIGHT - MIN_NHL_HEIGHT)\n",
    "    weight_score = 100 * (weight - MIN_NHL_WEIGHT) / (MAX_NHL_WEIGHT - MIN_NHL_WEIGHT)\n",
    "    \n",
    "    return f\"{height}:{height_score} {weight}:{weight_score}\"\n",
    "\n",
    "\n",
    "def extract_measurables_nba(row):\n",
    "    height = get_row_value(row, 'Ht.', str)  \n",
    "    weight = get_row_value(row, 'Wt.', float)  \n",
    "\n",
    "    if pd.isna(height) or pd.isna(weight):\n",
    "        return np.nan\n",
    "    \n",
    "    height = imp_height_to_metric_height(height)\n",
    "    \n",
    "    # Normalize height and weight\n",
    "    height_score = 100 * (height - MIN_NBA_HEIGHT) / (MAX_NBA_HEIGHT - MIN_NBA_HEIGHT)\n",
    "    weight_score = 100 * (weight - MIN_NBA_WEIGHT) / (MAX_NBA_WEIGHT - MIN_NBA_WEIGHT)\n",
    "    \n",
    "    return f\"{height}:{height_score} {weight}:{weight_score}\"\n",
    "\n",
    "def extract_measurables_nfl(row):\n",
    "    height = get_row_value(row, 'Ht.', str)  \n",
    "    weight = get_row_value(row, 'Wt.', float)  \n",
    "    \n",
    "    if pd.isna(height) or pd.isna(weight):\n",
    "        return np.nan\n",
    "    \n",
    "    height = imp_height_to_metric_height(height)\n",
    "    \n",
    "    # Normalize height and weight\n",
    "    height_score = 100 * (height - MIN_NFL_HEIGHT) / (MAX_NFL_HEIGHT - MIN_NFL_HEIGHT)\n",
    "    weight_score = 100 * (weight - MIN_NFL_WEIGHT) / (MAX_NFL_WEIGHT - MIN_NFL_WEIGHT)\n",
    "    \n",
    "    return f\"{height}:{height_score} {weight}:{weight_score}\"\n",
    "\n",
    "def extract_measurables_mlb(row):\n",
    "    height = get_row_value(row, 'Ht.', str)  \n",
    "    weight = get_row_value(row, 'Wt.', float)  \n",
    "    \n",
    "    if pd.isna(height) or pd.isna(weight):\n",
    "        return np.nan\n",
    "    \n",
    "    height = imp_height_to_metric_height(height)\n",
    "    \n",
    "    # Normalize height and weight\n",
    "    height_score = 100 * (height - MIN_MLB_HEIGHT) / (MAX_MLB_HEIGHT - MIN_MLB_HEIGHT)\n",
    "    weight_score = 100 * (weight - MIN_MLB_WEIGHT) / (MAX_MLB_WEIGHT - MIN_MLB_WEIGHT)\n",
    "    \n",
    "    return f\"{height}:{height_score} {weight}:{weight_score}\"\n",
    "\n",
    "def extract_measurables_mls(row):\n",
    "    height = get_row_value(row, 'Ht.', str)  \n",
    "    weight = get_row_value(row, 'Wt.', float)  \n",
    "    \n",
    "    if pd.isna(height) or pd.isna(weight):\n",
    "        return np.nan\n",
    "    \n",
    "    height = imp_height_to_metric_height(height)\n",
    "    \n",
    "    # Normalize height and weight\n",
    "    height_score = 100 * (height - MIN_MLS_HEIGHT) / (MAX_MLS_HEIGHT - MIN_MLS_HEIGHT)\n",
    "    weight_score = 100 * (weight - MIN_MLS_WEIGHT) / (MAX_MLS_WEIGHT - MIN_MLS_WEIGHT)\n",
    "    \n",
    "    return f\"{height}:{height_score} {weight}:{weight_score}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBA\n",
    "league = \"NBA\"\n",
    "\n",
    "league_files[league] = {\n",
    "    (\"Player\", \"Player-additional\", \"Season\"): extract_player_additional_generic,\n",
    "    'League': lambda row: league,\n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_nba,\n",
    "    \"Offensive Performance\": extract_offensive_performance_nba,\n",
    "    \"Defensive Performance\": extract_defensive_performance_nba,\n",
    "    \"Biometrics\": extract_measurables_nba,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFL\n",
    "league = \"NFL\"\n",
    "\n",
    "league_files[league] = {\n",
    "    (\"Player\", \"Player-additional\", \"Season\"): extract_player_additional_generic,\n",
    "    'League': lambda row: league,\n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_nfl,\n",
    "    \"Offensive Performance\": extract_offensive_performance_nfl,\n",
    "    \"Defensive Performance\": extract_defensive_performance_nfl,\n",
    "    \"Biometrics\": extract_measurables_nfl,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLB\n",
    "league = \"MLB\"\n",
    "\n",
    "league_files[league] = {    \n",
    "    (\"Player\", \"Player-additional\", \"Season\"): extract_player_additional_generic,\n",
    "    'League': lambda row: league,    \n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_mlb,\n",
    "    \"Offensive Performance\": extract_offensive_performance_mlb,\n",
    "    \"Defensive Performance\": extract_defensive_performance_mlb,\n",
    "    \"Biometrics\": extract_measurables_mlb,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHL\n",
    "league = \"NHL\"\n",
    "\n",
    "league_files[league] = {\n",
    "    (\"Player\", \"Player-additional\", \"Season\"): extract_player_additional_generic,\n",
    "    'League': lambda row: league,\n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_nhl,\n",
    "    \"Offensive Performance\": extract_offensive_performance_nhl,\n",
    "    \"Defensive Performance\": extract_defensive_performance_nhl,\n",
    "    \"Biometrics\": extract_measurables_nhl,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS\n",
    "league = \"MLS\"\n",
    "\n",
    "league_files[league] = {\n",
    "    (\"Player\", \"Player-additional\", \"Season\"): extract_player_additional_generic,\n",
    "    'League': lambda row: league,\n",
    "    ('Home Continent', 'Home Country', 'Home City', \"Migration Difficulty\"): extract_location_generic,\n",
    "    \"Overall Value\": extract_overall_value_mls,\n",
    "    \"Offensive Performance\": extract_offensive_performance_mls,\n",
    "    \"Defensive Performance\": extract_defensive_performance_mls,\n",
    "    \"Biometrics\": extract_measurables_mls,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of column names to update\n",
    "columns_to_update = None\n",
    "# columns_to_update = ['Player']\n",
    "# columns_to_update = [(\"Player\", \"Player-additional\", \"Season\")]\n",
    "# columns_to_update = [('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')]\n",
    "# columns_to_update = ['Overall Value']\n",
    "# columns_to_update = ['Offensive Performance', 'Defensive Performance']\n",
    "# columns_to_update = ['Overall Value', 'Offensive Performance', 'Defensive Performance', 'Biometrics']\n",
    "# columns_to_update = ['Biometrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ordered_set import OrderedSet\n",
    "\n",
    "def adaptive_merge(dataframes):\n",
    "    \n",
    "  merged_df = pd.DataFrame(columns=['Player', 'Player-additional', 'Season'])\n",
    "  broadcast_columns = ['Player', 'Birth Location', 'Ht.', 'Wt.', 'BMI', 'Pos', 'Country', 'State']\n",
    "  for df in dataframes:\n",
    "    if 'Season' in df.columns:\n",
    "        merge_keys = ['Player-additional', 'Season']\n",
    "    else:\n",
    "        merge_keys = ['Player-additional']\n",
    "        # Treat all columns as broadcast columns if the data does not contain a 'Season' column\n",
    "        # broadcast_columns.extend([col for col in df.columns if col not in ['Player-additional', 'Season']])\n",
    "\n",
    "    # Performing the merge with suffixes for overlapping column names\n",
    "    merged_df = pd.merge(merged_df, df, on=merge_keys, how='outer', suffixes=('_x', '_y'))\n",
    "\n",
    "    final_columns = OrderedSet()\n",
    "    for column in merged_df.columns:\n",
    "      if \"_\" in column:\n",
    "        col, ind = column.rsplit(\"_\", 1)\n",
    "      else:\n",
    "        col, ind = column, \"\"\n",
    "        \n",
    "      final_columns.add(col)\n",
    "      \n",
    "      # Filter out unique columns\n",
    "      if ind != \"x\":\n",
    "        continue\n",
    "\n",
    "      # Merge all duplicate columns: '_x' columns with their '_y' columns\n",
    "      merged_df[col] = merged_df[col + \"_x\"].combine_first(merged_df[col + \"_y\"])\n",
    "\n",
    "    # Remove duplicate columns from final df\n",
    "    merged_df = merged_df[list(final_columns)]\n",
    "\n",
    "  # Fill in missing values by duplicating values for all seasons\n",
    "  for broadcast_col in broadcast_columns:\n",
    "    if broadcast_col in merged_df.columns:\n",
    "      merged_df[broadcast_col] = merged_df.groupby('Player-additional')[broadcast_col].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
    "\n",
    "  merged_df.drop_duplicates(inplace=True)\n",
    "  # merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  return merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing league: NBA\n",
      "\tReading file: NBA_foreign.csv\n",
      "\tReading file: NBA_height.csv\n",
      "\tReading file: NBA_weight.csv\n",
      "\t\tSynthesizing '('Player', 'Player-additional', 'Season')'...\n",
      "\t\tSynthesizing 'League'...\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\t\tSynthesizing 'Overall Value'...\n",
      "\t\tSynthesizing 'Offensive Performance'...\n",
      "\t\tSynthesizing 'Defensive Performance'...\n",
      "\t\tSynthesizing 'Biometrics'...\n",
      "\tSaving NBA data to ./formatted/NBA_formatted.csv...\n",
      "\n",
      "Processing league: NFL\n",
      "\tReading file: NFL_all.csv\n",
      "\tReading file: NFL_measure.csv\n",
      "\tReading file: NFL_births.csv\n",
      "\t\tSynthesizing '('Player', 'Player-additional', 'Season')'...\n",
      "\t\tSynthesizing 'League'...\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\t\tSynthesizing 'Overall Value'...\n",
      "\t\tSynthesizing 'Offensive Performance'...\n",
      "\t\tSynthesizing 'Defensive Performance'...\n",
      "\t\tSynthesizing 'Biometrics'...\n",
      "\tSaving NFL data to ./formatted/NFL_formatted.csv...\n",
      "\n",
      "Processing league: MLB\n",
      "\tReading file: MLB_foreign.csv\n",
      "\tReading file: MLB_measure.csv\n",
      "\t\tSynthesizing '('Player', 'Player-additional', 'Season')'...\n",
      "\t\tSynthesizing 'League'...\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\t\tSynthesizing 'Overall Value'...\n",
      "\t\tSynthesizing 'Offensive Performance'...\n",
      "\t\tSynthesizing 'Defensive Performance'...\n",
      "\t\tSynthesizing 'Biometrics'...\n",
      "\tSaving MLB data to ./formatted/MLB_formatted.csv...\n",
      "\n",
      "Processing league: NHL\n",
      "\tReading file: NHL_foreign.csv\n",
      "\tReading file: NHL_measure.csv\n",
      "\t\tSynthesizing '('Player', 'Player-additional', 'Season')'...\n",
      "\t\tSynthesizing 'League'...\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\t\tSynthesizing 'Overall Value'...\n",
      "\t\tSynthesizing 'Offensive Performance'...\n",
      "\t\tSynthesizing 'Defensive Performance'...\n",
      "\t\tSynthesizing 'Biometrics'...\n",
      "\tSaving NHL data to ./formatted/NHL_formatted.csv...\n",
      "\n",
      "Processing league: MLS\n",
      "\tReading file: MLS_foreign.csv\n",
      "\t\tSynthesizing '('Player', 'Player-additional', 'Season')'...\n",
      "\t\tSynthesizing 'League'...\n",
      "\t\tSynthesizing '('Home Continent', 'Home Country', 'Home City', 'Migration Difficulty')'...\n",
      "\t\tSynthesizing 'Overall Value'...\n",
      "\t\tSynthesizing 'Offensive Performance'...\n",
      "\t\tSynthesizing 'Defensive Performance'...\n",
      "\t\tSynthesizing 'Biometrics'...\n",
      "\tSaving MLS data to ./formatted/MLS_formatted.csv...\n",
      "\n",
      "Saving all leagues combined data to ./formatted/combined_formatted.csv...\n",
      "\n",
      "The following columns were not accessible:\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output directory exists\n",
    "output_directory = \"./formatted\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Initialize a list to store paths to the formatted CSV files\n",
    "formatted_csv_paths = []\n",
    "\n",
    "for league, filenames in files.items():\n",
    "    print(f\"Processing league: {league}\")\n",
    "    # Load the existing formatted CSV if it exists\n",
    "    output_path = os.path.join(output_directory, f\"{league}_formatted.csv\")\n",
    "    # Create df_final, which holds all processed final_columns\n",
    "    if os.path.exists(output_path):\n",
    "        df_final = pd.read_csv(output_path)\n",
    "    else:\n",
    "        # Initialize empty DataFrame with final columns if no file exists\n",
    "        df_final = pd.DataFrame(columns=final_columns)\n",
    "\n",
    "    dataframes = []\n",
    "    for filename in filenames:\n",
    "        print(f\"\\tReading file: {filename}\")\n",
    "        df = pd.read_csv(filename, low_memory=False)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    if len(dataframes) > 1:\n",
    "        df_merged = adaptive_merge(dataframes) # Contains all columns of all dataframes for one league\n",
    "    else:\n",
    "        df_merged = dataframes[0]\n",
    "\n",
    "    # Remove all non-US for NFL\n",
    "    if league == \"NFL\":\n",
    "        df_merged = df_merged[df_merged['Country'] != 'USA']\n",
    "\n",
    "    # Process the merged DataFrame with the functions defined for the league\n",
    "    df_temp = pd.DataFrame() # Only contains processed columns, e.g., Offensive Performance, not all final_columns\n",
    "    for column_names, function in league_files[league].items():\n",
    "        # Skip columns that are not in the list of columns to update\n",
    "        if columns_to_update and column_names not in columns_to_update:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\t\\tSynthesizing '{column_names}'...\")\n",
    "        column_names = list(column_names) if isinstance(column_names, tuple) else column_names\n",
    "        df_temp[column_names] = df_merged.apply(function, axis=1, result_type='expand')\n",
    "\n",
    "    # Ensure the merged DataFrame has the columns from the processed data\n",
    "    df_final[df_temp.columns] = df_temp\n",
    "    df_final = df_final.dropna(how='all')  # Drop rows with all NA values\n",
    "\n",
    "    # Save inaccesible columns with league information and then clear unknown for next league\n",
    "    inaccessable_columns[league], inaccessable_columns[\"Unknown\"] = inaccessable_columns[\"Unknown\"].copy(), set()\n",
    "\n",
    "    # Save to a new CSV file for the entire league\n",
    "    print(f\"\\tSaving {league} data to {output_path}...\")\n",
    "    save_geo_dataframes()\n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    formatted_csv_paths.append(output_path)\n",
    "    print()\n",
    "\n",
    "# Combine all formatted CSV files into one big DataFrame and save it\n",
    "combined_df = pd.concat([pd.read_csv(f) for f in formatted_csv_paths], ignore_index=True)\n",
    "combined_output_path = os.path.join(output_directory, \"combined_formatted.csv\")\n",
    "print(f\"Saving all leagues combined data to {combined_output_path}...\")\n",
    "combined_df.to_csv(combined_output_path, index=False)\n",
    "\n",
    "print(f\"\\nThe following columns were not accessible:\")\n",
    "for league, cols in inaccessable_columns.items():\n",
    "    if cols:\n",
    "        print(f\"{league}:\", cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Player-additional</th>\n",
       "      <th>League</th>\n",
       "      <th>Season</th>\n",
       "      <th>Home Continent</th>\n",
       "      <th>Home Country</th>\n",
       "      <th>Home City</th>\n",
       "      <th>Overall Value</th>\n",
       "      <th>Offensive Performance</th>\n",
       "      <th>Defensive Performance</th>\n",
       "      <th>Biometrics</th>\n",
       "      <th>Migration Difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Joe Abbey</td>\n",
       "      <td>AbbeJo20</td>\n",
       "      <td>NFL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0:18.75</td>\n",
       "      <td>0:0.0 0:0.9083728278041074 0:0.072979383324210...</td>\n",
       "      <td>0:0.0 0:0.0 0:0.0 0:0.0 0:0.0 0.0:0.0 0.0:0.0 ...</td>\n",
       "      <td>185.4:52.136752136752136 202.0:31.800766283524904</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Fay Abbott</td>\n",
       "      <td>AbboFa20</td>\n",
       "      <td>NFL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0:18.75</td>\n",
       "      <td>0:0.0 0:0.9083728278041074 0:0.072979383324210...</td>\n",
       "      <td>0:0.0 0:0.0 0:0.0 0:0.0 0:0.0 0.0:0.0 0.0:0.0 ...</td>\n",
       "      <td>172.7:30.427350427350397 182.0:24.137931034482758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Hamza Abdullah</td>\n",
       "      <td>AbduHa20</td>\n",
       "      <td>NFL</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0:18.75</td>\n",
       "      <td>0:0.0 0:0.9083728278041074 0:0.072979383324210...</td>\n",
       "      <td>1:0.5 0.0:0.0 0:0.0 0:0.0 0:0.0 1.0:7.46268656...</td>\n",
       "      <td>188.0:56.58119658119657 216.0:37.16475095785441</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Hamza Abdullah</td>\n",
       "      <td>AbduHa20</td>\n",
       "      <td>NFL</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0:21.875</td>\n",
       "      <td>0:0.0 0:0.9083728278041074 0:0.072979383324210...</td>\n",
       "      <td>12:6.0 0.0:0.0 0:0.0 1:10.0 0:0.0 1.0909090909...</td>\n",
       "      <td>188.0:56.58119658119657 216.0:37.16475095785441</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Hamza Abdullah</td>\n",
       "      <td>AbduHa20</td>\n",
       "      <td>NFL</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0:28.125</td>\n",
       "      <td>0:0.0 0:0.9083728278041074 0:0.072979383324210...</td>\n",
       "      <td>48:24.0 0.0:0.0 0:0.0 2:20.0 5:16.129032258064...</td>\n",
       "      <td>188.0:56.58119658119657 216.0:37.16475095785441</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39731</th>\n",
       "      <td>Julio Cascante</td>\n",
       "      <td>cc120b4f</td>\n",
       "      <td>MLS</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-16:25.581395348837212</td>\n",
       "      <td>2:5.882352941176471 1:4.761904761904762 0.0645...</td>\n",
       "      <td>19:19.19191919191919 29:24.166666666666668 0.6...</td>\n",
       "      <td>182.9:61.135371179039325 176.0:50.413223140495866</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39736</th>\n",
       "      <td>Alvas Powell</td>\n",
       "      <td>2f91124d</td>\n",
       "      <td>MLS</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17:24.418604651162788</td>\n",
       "      <td>0:0.0 0:0.0 0.0:0.0 0.0:0.0</td>\n",
       "      <td>13:13.131313131313131 11:9.166666666666666 1.0...</td>\n",
       "      <td>182.9:61.135371179039325 165.0:41.32231404958678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39753</th>\n",
       "      <td>Hwang In-beom</td>\n",
       "      <td>92fa5d28</td>\n",
       "      <td>MLS</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-18:23.25581395348837</td>\n",
       "      <td>3:8.823529411764707 3:14.285714285714286 0.088...</td>\n",
       "      <td>30:30.303030303030305 54:45.0 0.88235294117647...</td>\n",
       "      <td>175.3:44.54148471615723 154.0:32.231404958677686</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39772</th>\n",
       "      <td>Luca Petrasso</td>\n",
       "      <td>fafeaf3d</td>\n",
       "      <td>MLS</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-19:22.093023255813954</td>\n",
       "      <td>0:0.0 2:9.523809523809524 0.0:0.0 0.0869565217...</td>\n",
       "      <td>13:13.131313131313131 18:15.0 0.56521739130434...</td>\n",
       "      <td>177.8:50.00000000000003 168.0:43.80165289256198</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39791</th>\n",
       "      <td>Tony Alfaro</td>\n",
       "      <td>4118b12a</td>\n",
       "      <td>MLS</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-23:17.441860465116278</td>\n",
       "      <td>0:0.0 1:4.761904761904762 0.0:0.0 0.0555555555...</td>\n",
       "      <td>7:7.070707070707071 14:11.666666666666666 0.38...</td>\n",
       "      <td>188.0:72.27074235807862 190.0:61.98347107438016</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18409 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Player Player-additional League  Season Home Continent  \\\n",
       "64          Joe Abbey          AbbeJo20    NFL     NaN            NaN   \n",
       "65         Fay Abbott          AbboFa20    NFL     NaN            NaN   \n",
       "67     Hamza Abdullah          AbduHa20    NFL  2005.0            NaN   \n",
       "68     Hamza Abdullah          AbduHa20    NFL  2006.0            NaN   \n",
       "69     Hamza Abdullah          AbduHa20    NFL  2007.0            NaN   \n",
       "...               ...               ...    ...     ...            ...   \n",
       "39731  Julio Cascante          cc120b4f    MLS    2021            NaN   \n",
       "39736    Alvas Powell          2f91124d    MLS    2019            NaN   \n",
       "39753   Hwang In-beom          92fa5d28    MLS    2019            NaN   \n",
       "39772   Luca Petrasso          fafeaf3d    MLS    2022            NaN   \n",
       "39791     Tony Alfaro          4118b12a    MLS    2022            NaN   \n",
       "\n",
       "      Home Country Home City            Overall Value  \\\n",
       "64             NaN       NaN                 0:18.75    \n",
       "65             NaN       NaN                 0:18.75    \n",
       "67             NaN       NaN               0.0:18.75    \n",
       "68             NaN       NaN              1.0:21.875    \n",
       "69             NaN       NaN              3.0:28.125    \n",
       "...            ...       ...                      ...   \n",
       "39731          NaN       NaN  -16:25.581395348837212    \n",
       "39736          NaN       NaN  -17:24.418604651162788    \n",
       "39753          NaN       NaN   -18:23.25581395348837    \n",
       "39772          NaN       NaN  -19:22.093023255813954    \n",
       "39791          NaN       NaN  -23:17.441860465116278    \n",
       "\n",
       "                                   Offensive Performance  \\\n",
       "64     0:0.0 0:0.9083728278041074 0:0.072979383324210...   \n",
       "65     0:0.0 0:0.9083728278041074 0:0.072979383324210...   \n",
       "67     0:0.0 0:0.9083728278041074 0:0.072979383324210...   \n",
       "68     0:0.0 0:0.9083728278041074 0:0.072979383324210...   \n",
       "69     0:0.0 0:0.9083728278041074 0:0.072979383324210...   \n",
       "...                                                  ...   \n",
       "39731  2:5.882352941176471 1:4.761904761904762 0.0645...   \n",
       "39736                        0:0.0 0:0.0 0.0:0.0 0.0:0.0   \n",
       "39753  3:8.823529411764707 3:14.285714285714286 0.088...   \n",
       "39772  0:0.0 2:9.523809523809524 0.0:0.0 0.0869565217...   \n",
       "39791  0:0.0 1:4.761904761904762 0.0:0.0 0.0555555555...   \n",
       "\n",
       "                                   Defensive Performance  \\\n",
       "64     0:0.0 0:0.0 0:0.0 0:0.0 0:0.0 0.0:0.0 0.0:0.0 ...   \n",
       "65     0:0.0 0:0.0 0:0.0 0:0.0 0:0.0 0.0:0.0 0.0:0.0 ...   \n",
       "67     1:0.5 0.0:0.0 0:0.0 0:0.0 0:0.0 1.0:7.46268656...   \n",
       "68     12:6.0 0.0:0.0 0:0.0 1:10.0 0:0.0 1.0909090909...   \n",
       "69     48:24.0 0.0:0.0 0:0.0 2:20.0 5:16.129032258064...   \n",
       "...                                                  ...   \n",
       "39731  19:19.19191919191919 29:24.166666666666668 0.6...   \n",
       "39736  13:13.131313131313131 11:9.166666666666666 1.0...   \n",
       "39753  30:30.303030303030305 54:45.0 0.88235294117647...   \n",
       "39772  13:13.131313131313131 18:15.0 0.56521739130434...   \n",
       "39791  7:7.070707070707071 14:11.666666666666666 0.38...   \n",
       "\n",
       "                                              Biometrics Migration Difficulty  \n",
       "64     185.4:52.136752136752136 202.0:31.800766283524904                  NaN  \n",
       "65     172.7:30.427350427350397 182.0:24.137931034482758                  NaN  \n",
       "67       188.0:56.58119658119657 216.0:37.16475095785441                  NaN  \n",
       "68       188.0:56.58119658119657 216.0:37.16475095785441                  NaN  \n",
       "69       188.0:56.58119658119657 216.0:37.16475095785441                  NaN  \n",
       "...                                                  ...                  ...  \n",
       "39731  182.9:61.135371179039325 176.0:50.413223140495866                  NaN  \n",
       "39736   182.9:61.135371179039325 165.0:41.32231404958678                  NaN  \n",
       "39753   175.3:44.54148471615723 154.0:32.231404958677686                  NaN  \n",
       "39772    177.8:50.00000000000003 168.0:43.80165289256198                  NaN  \n",
       "39791    188.0:72.27074235807862 190.0:61.98347107438016                  NaN  \n",
       "\n",
       "[18409 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Player-additional</th>\n",
       "      <th>League</th>\n",
       "      <th>Season</th>\n",
       "      <th>Home Continent</th>\n",
       "      <th>Home Country</th>\n",
       "      <th>Home City</th>\n",
       "      <th>Overall Value</th>\n",
       "      <th>Offensive Performance</th>\n",
       "      <th>Defensive Performance</th>\n",
       "      <th>Biometrics</th>\n",
       "      <th>Migration Difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Player, Player-additional, League, Season, Home Continent, Home Country, Home City, Overall Value, Offensive Performance, Defensive Performance, Biometrics, Migration Difficulty]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df['Player-additional'] == \"mingya01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
